{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# ------------------------------------------define logging and working directory\n",
    "from ProjectRoot import change_wd_to_project_root\n",
    "change_wd_to_project_root()\n",
    "from src.utils.Tensorflow_helper import choose_gpu_by_id\n",
    "# ------------------------------------------define GPU id/s to use, if given\n",
    "GPU_IDS = '0,1'\n",
    "GPUS = choose_gpu_by_id(GPU_IDS)\n",
    "print(GPUS)\n",
    "# ------------------------------------------jupyter magic config\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# ------------------------------------------ import helpers\n",
    "# this should import glob, os, and many other standard libs\n",
    "# local imports\n",
    "from src.utils.Notebook_imports import *\n",
    "from src.utils.Utils_io import Console_and_file_logger, init_config\n",
    "\n",
    "# import external libs\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "EXPERIMENT = 'cv_histmatchchoice_newdata/8_64_64__8_2_2_4tenc_conv1_MSE_NOnorm_augshiftrot_taug_5_batch8'\n",
    "#EXPERIMENT = 'baseline_label_transpose_smooth05/36_5_BiLSTM32_NoBn_conv5_size1_CCE_NOphaseaug_NOaug_b8'\n",
    "# EXPERIMENT = 'mased_scores/36_5_BiLSTM32_NoBn_conv5_size1_CCE_NOphaseaug_shift_rotate_reflectbordersgridaug'\n",
    "timestemp = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")) # ad a timestep to each project to make repeated experiments unique\n",
    "\n",
    "EXPERIMENTS_ROOT = 'exp/'\n",
    "EXP_PATH = os.path.join(EXPERIMENTS_ROOT, EXPERIMENT, timestemp)\n",
    "MODEL_PATH = os.path.join(EXP_PATH, 'model', )\n",
    "TENSORBOARD_PATH = os.path.join(EXP_PATH, 'tensorboard_logs')\n",
    "CONFIG_PATH = os.path.join(EXP_PATH,'config')\n",
    "HISTORY_PATH = os.path.join(EXP_PATH, 'history')\n",
    "ensure_dir(MODEL_PATH)\n",
    "ensure_dir(TENSORBOARD_PATH)\n",
    "ensure_dir(CONFIG_PATH)\n",
    "ensure_dir(HISTORY_PATH)\n",
    "\n",
    "# define the input data paths and fold \n",
    "# first to the 4D Nrrd files, \n",
    "# second to a dataframe with a mapping of the Fold-number\n",
    "# Finally the path to the metadata\n",
    "DATA_PATH_SAX = '/mnt/ssd/data/gcn/02_imported_4D_unfiltered/SAX/'\n",
    "DF_FOLDS = '/mnt/ssd/data/gcn/02_imported_4D_unfiltered/df_kfold.csv'\n",
    "DF_META = '/mnt/ssd/data/gcn/02_imported_4D_unfiltered/SAx_3D_dicomTags_phase'\n",
    "FOLD = 0\n",
    "\n",
    "# General params\n",
    "SEED = 42 # define a seed for the generator shuffle\n",
    "BATCHSIZE = 8 # 32, 64, 24, 16, 1 for 3D use: 4\n",
    "GENERATOR_WORKER = BATCHSIZE # if not set, use batchsize\n",
    "EPOCHS = 100\n",
    "\n",
    "DIM = [8, 64, 64] # network input shape for spacing of 3, (z,y,x)\n",
    "T_SHAPE = 36\n",
    "SPACING = [8, 3, 3] # if resample, resample to this spacing, (z,y,x)\n",
    "\n",
    "# Model params\n",
    "DEPTH = 4 # depth of the encoder\n",
    "FILTERS = 32 # initial number of filters, will be doubled after each downsampling block\n",
    "M_POOL = [1, 2, 2]# size of max-pooling used for downsampling and upsampling\n",
    "F_SIZE = [3, 3, 3] # conv filter size\n",
    "BN_FIRST = False # decide if batch normalisation between conv and activation or afterwards, after nonlinear generalised better\n",
    "BATCH_NORMALISATION = True # apply BN or not\n",
    "PAD = 'same' # padding strategy of the conv layers\n",
    "KERNEL_INIT = 'he_normal' # conv weight initialisation\n",
    "OPTIMIZER = 'adam' # Adam, Adagrad, RMSprop, Adadelta,  # https://keras.io/optimizers/\n",
    "ACTIVATION = 'relu' # tf.keras.layers.LeakyReLU(), relu or any other non linear activation function\n",
    "FINAL_ACTIVATION = 'relu' # one of : 'relu', 'softmax'\n",
    "LOSS = 'meandiff' # one of 'mse', 'cce' or 'meandiff'\n",
    "LEARNING_RATE = 1e-4 # start with a huge lr to converge fast\n",
    "REDUCE_LR_ON_PLAEAU_PATIENCE = 5\n",
    "DECAY_FACTOR = 0.7 # Define a learning rate decay for the ReduceLROnPlateau callback\n",
    "POLY_LR_DECAY = False\n",
    "MIN_LR = 1e-12 # minimal lr, smaller lr does not improve the model\n",
    "DROPOUT_min = 0.3 # lower dropout at the shallow layers\n",
    "DROPOUT_max = 0.5 # higher dropout at the deep layers\n",
    "\n",
    "# Callback params\n",
    "MONITOR_FUNCTION = 'loss'\n",
    "MONITOR_MODE = 'min'\n",
    "SAVE_MODEL_FUNCTION = 'loss'\n",
    "SAVE_MODEL_MODE = 'min'\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "SAVE_LEARNING_PROGRESS_AS_TF = True\n",
    "\n",
    "# Generator and Augmentation params\n",
    "BORDER_MODE = cv2.BORDER_REFLECT_101 # border mode for the data generation\n",
    "IMG_INTERPOLATION = cv2.INTER_LINEAR # image interpolation in the genarator\n",
    "MSK_INTERPOLATION = cv2.INTER_NEAREST # mask interpolation in the generator\n",
    "AUGMENT = True # a compose of 2D augmentation (grid distortion, 90degree rotation, brightness and shift)\n",
    "AUGMENT_PROB = 0.8\n",
    "AUGMENT_PHASES = True\n",
    "AUGMENT_PHASES_RANGE = (-5,5)\n",
    "REPEAT_ONEHOT = True\n",
    "SHUFFLE = True\n",
    "RESAMPLE = True\n",
    "HIST_MATCHING = True\n",
    "SCALER = 'MinMax' # MinMax, Standard or Robust\n",
    "# We define 5 target phases and a background phase for the pad/empty volumes \n",
    "PHASES = len(['ED#', 'MS#', 'ES#', 'PF#', 'MD#']) # skipped 'pad backround manually added', due to repeating\n",
    "TARGET_SMOOTHING = True\n",
    "SMOOTHING_KERNEL_SIZE = 12\n",
    "SMOOTHING_LOWER_BORDER = 1\n",
    "SMOOTHING_UPPER_BORDER = 5\n",
    "SMOOTHING_WEIGHT_CORRECT = 10\n",
    "config = init_config(config=locals(), save=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from src.models.train_model import train_fold\n",
    "\n",
    "folds = [i for i in range(0, 1, 1)]\n",
    "\n",
    "for f in folds:\n",
    "    info('starting fold: {}'.format(f))\n",
    "    config_ = config.copy()\n",
    "    config_['FOLD'] = f\n",
    "    train_fold(config_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "def foo(*kwargs):\n",
    "    print(kwargs)\n",
    "    \n",
    "    \n",
    "cfg = {}\n",
    "cfg['1'] = 'a'\n",
    "\n",
    "foo(cfg)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcmr",
   "language": "python",
   "name": "dcmr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
