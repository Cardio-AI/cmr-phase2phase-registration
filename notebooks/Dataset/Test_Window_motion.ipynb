{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for root_dir and set working directory\n",
      "Working directory set to: /mnt/ssd/git/dynamic-cmr-models\n",
      "['/gpu:0', '/gpu:1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:31,501 INFO -------------------- Start --------------------\n",
      "2021-05-21 11:58:31,502 INFO Working directory: /mnt/ssd/git/dynamic-cmr-models.\n",
      "2021-05-21 11:58:31,502 INFO Log file: ./logs/temp/windowmotion.log\n",
      "2021-05-21 11:58:31,502 INFO Log level for console: INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__K': None, 'GPU_IDS': '0,1', 'GPUS': ['/gpu:0', '/gpu:1'], 'SEED': 42, 'EXPERIMENT': 'temp/windowmotion', 'EXPERIMENTS_ROOT': 'exp/', 'EXP_PATH': 'exp/temp/windowmotion/2021-05-21_11_58', 'MODEL_PATH': 'exp/temp/windowmotion/2021-05-21_11_58/model', 'TENSORBOARD_PATH': 'exp/temp/windowmotion/2021-05-21_11_58/tensorboard_logs', 'CONFIG_PATH': 'exp/temp/windowmotion/2021-05-21_11_58/config', 'HISTORY_PATH': 'exp/temp/windowmotion/2021-05-21_11_58/history', 'DATA_PATH_SAX': '/mnt/ssd/data/gcn/02_imported_4D_unfiltered/sax/', 'DF_FOLDS': '/mnt/ssd/data/gcn/metadata/df_kfold.csv', 'DF_META': '/mnt/ssd/data/gcn/metadata/SAx_3D_dicomTags_phase', 'FOLD': 0, 'BATCHSIZE': 2, 'GENERATOR_WORKER': 2, 'DIM': [64, 128, 128], 'SPACING': [2, 2, 2], 'T_SPACING': 55, 'DEPTH': 3, 'FILTERS': 16, 'M_POOL': [2, 2, 2], 'F_SIZE': [3, 3, 3], 'MASK_VALUES': [1, 2, 3], 'MASK_CLASSES': 3, 'MASKING_IMAGE': False, 'MASKING_VALUES': [1, 2, 3], 'BORDER_MODE': 4, 'IMG_INTERPOLATION': 1, 'MSK_INTERPOLATION': 0, 'AUGMENT': True, 'AUGMENT_PROB': 0.8, 'SHUFFLE': True, 'RESAMPLE': True, 'AUGMENT_TEMP': False, 'AUGMENT_TEMP_RANGE': (-2, 2), 'RESAMPLE_T': True, 'HIST_MATCHING': False, 'SCALER': 'Standard', 'TENSORBOARD_LOG_DIR': 'exp/temp/windowmotion/2021-05-21_11_58/tensorboard_logs'}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------define logging and working directory\n",
    "from ProjectRoot import change_wd_to_project_root\n",
    "change_wd_to_project_root()\n",
    "\n",
    "from src.utils.Tensorflow_helper import choose_gpu_by_id\n",
    "# ------------------------------------------define GPU id/s to use\n",
    "GPU_IDS = '0,1'\n",
    "GPUS = choose_gpu_by_id(GPU_IDS)\n",
    "print(GPUS)\n",
    "\n",
    "# this should import glob, os, and many other standard libs\n",
    "from src.utils.Notebook_imports import *\n",
    "\n",
    "# local imports\n",
    "from src.utils.Utils_io import Console_and_file_logger, init_config\n",
    "from src.visualization.Visualize import show_2D_or_3D\n",
    "\n",
    "# import external libs\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------jupyter magic config\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "EXPERIMENT = 'temp/windowmotion'\n",
    "timestemp = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")) # ad a timestep to each project to make repeated experiments unique\n",
    "\n",
    "EXPERIMENTS_ROOT = 'exp/'\n",
    "EXP_PATH = os.path.join(EXPERIMENTS_ROOT, EXPERIMENT, timestemp)\n",
    "MODEL_PATH = os.path.join(EXP_PATH, 'model', )\n",
    "TENSORBOARD_PATH = os.path.join(EXP_PATH, 'tensorboard_logs')\n",
    "CONFIG_PATH = os.path.join(EXP_PATH,'config')\n",
    "HISTORY_PATH = os.path.join(EXP_PATH, 'history')\n",
    "ensure_dir(MODEL_PATH)\n",
    "ensure_dir(TENSORBOARD_PATH)\n",
    "ensure_dir(CONFIG_PATH)\n",
    "ensure_dir(HISTORY_PATH)\n",
    "\n",
    "# define the input data paths and fold \n",
    "# first to the 4D Nrrd files, \n",
    "# second to a dataframe with a mapping of the Fold-number\n",
    "# Finally the path to the metadata\n",
    "DATA_PATH_SAX = '/mnt/ssd/data/gcn/02_imported_4D_unfiltered/sax/'\n",
    "DF_FOLDS = '/mnt/ssd/data/gcn/metadata/df_kfold.csv'\n",
    "DF_META = '/mnt/ssd/data/gcn/metadata/SAx_3D_dicomTags_phase'\n",
    "FOLD = 0\n",
    "\n",
    "# ------------------------------------------generator hyperparameters\n",
    "SEED = 42 # define a seed for the generator shuffle\n",
    "BATCHSIZE = 2 # 32, 64, 24, 16, 1 for 3D use: 4\n",
    "GENERATOR_WORKER = BATCHSIZE # if not set, use batchsize\n",
    "DIM = [64, 128, 128] # network input params for spacing of 3, (z,y,x)\n",
    "SPACING = [2, 2, 2] # if resample, resample to this spacing, (z,y,x)\n",
    "T_SPACING = 55\n",
    "DEPTH = 3 # depth of the encoder\n",
    "FILTERS = 16 # initial number of filters, will be doubled after each downsampling block\n",
    "M_POOL = [2, 2, 2]# size of max-pooling used for downsampling and upsampling\n",
    "F_SIZE = [3, 3, 3] # conv filter size\n",
    "# maybe we can drop this channel?\n",
    "#IMG_CHANNELS = 1 # Currently our model needs that image channel\n",
    "MASK_VALUES = [1, 2, 3]  #channel order: Background, RV, MYO, LV\n",
    "MASK_CLASSES = len(MASK_VALUES) # no of labels\n",
    "MASKING_IMAGE = False\n",
    "MASKING_VALUES = [1,2,3]\n",
    "BORDER_MODE = cv2.BORDER_REFLECT_101 # border mode for the data generation\n",
    "IMG_INTERPOLATION = cv2.INTER_LINEAR # image interpolation in the genarator\n",
    "MSK_INTERPOLATION = cv2.INTER_NEAREST # mask interpolation in the generator\n",
    "AUGMENT = True # a compose of 2D augmentation (grid distortion, 90degree rotation, brightness and shift)\n",
    "AUGMENT_PROB = 0.8\n",
    "SHUFFLE = True\n",
    "RESAMPLE = True\n",
    "AUGMENT_TEMP = False\n",
    "AUGMENT_TEMP_RANGE = (-2,2)\n",
    "RESAMPLE_T = True\n",
    "HIST_MATCHING = False\n",
    "SCALER = 'Standard' # MinMax Standard or Robust\n",
    "\n",
    "Console_and_file_logger(EXPERIMENT, logging.INFO)\n",
    "config = init_config(config=locals(), save=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:31,584 INFO no files found, try to load with clean.nrrd/mask.nrrd pattern\n",
      "2021-05-21 11:58:31,585 INFO searched in: /mnt/ssd/data/gcn/02_imported_4D_unfiltered/sax/\n",
      "2021-05-21 11:58:31,593 INFO Found 278 images/masks in /mnt/ssd/data/gcn/02_imported_4D_unfiltered/sax/\n",
      "2021-05-21 11:58:31,593 INFO Patients train: 209\n",
      "2021-05-21 11:58:31,604 INFO Selected 209 of 278 files with 209 of 279 patients for training fold 0\n",
      "2021-05-21 11:58:31,604 INFO SAX train CMR: 209, SAX train masks: 209\n",
      "2021-05-21 11:58:31,605 INFO SAX val CMR: 69, SAX val masks: 69\n"
     ]
    }
   ],
   "source": [
    "# Load SAX volumes\n",
    "from src.data.Dataset import get_trainings_files\n",
    "x_train_sax, y_train_sax, x_val_sax, y_val_sax =  get_trainings_files(data_path=DATA_PATH_SAX,path_to_folds_df=DF_FOLDS, fold=FOLD)\n",
    "logging.info('SAX train CMR: {}, SAX train masks: {}'.format(len(x_train_sax), len(y_train_sax)))\n",
    "logging.info('SAX val CMR: {}, SAX val masks: {}'.format(len(x_val_sax), len(y_val_sax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:31,760 INFO Create DataGenerator\n",
      "2021-05-21 11:58:31,761 INFO Datagenerator created with: \n",
      " shape: [64, 128, 128]\n",
      " spacing: [2, 2, 2]\n",
      " batchsize: 2\n",
      " Scaler: Standard\n",
      " Images: 209 \n",
      " Augment: True \n",
      " Thread workers: 2\n",
      "2021-05-21 11:58:31,761 INFO Data will be augmented (shift,scale and rotate) with albumentation\n",
      "2021-05-21 11:58:31,775 INFO params of generator:\n",
      "2021-05-21 11:58:31,776 INFO [('MASKING_IMAGE', False), ('SINGLE_OUTPUT', False), ('MASKING_VALUES', [1, 2, 3]), ('DEBUG_MODE', False), ('SCALER', 'Standard'), ('AUGMENT', True), ('SHUFFLE', True), ('RESAMPLE', True), ('SPACING', [2, 2, 2]), ('SEED', 42), ('DIM', [64, 128, 128]), ('BATCHSIZE', 2), ('MASK_VALUES', [1, 2, 3]), ('N_CLASSES', 3), ('MAX_WORKERS', 2), ('T_SPACING', 55), ('PHASES', 5), ('HIST_MATCHING', False), ('IMG_INTERPOLATION', 1), ('MSK_INTERPOLATION', 0), ('AUGMENT_TEMP', False), ('RESAMPLE_T', True), ('ISACDC', False), ('METADATA_FILE', '/mnt/ssd/data/gcn/metadata/SAx_3D_dicomTags_phase')]\n",
      "2021-05-21 11:58:31,776 INFO Create DataGenerator\n",
      "2021-05-21 11:58:31,777 INFO Datagenerator created with: \n",
      " shape: [64, 128, 128]\n",
      " spacing: [2, 2, 2]\n",
      " batchsize: 2\n",
      " Scaler: Standard\n",
      " Images: 69 \n",
      " Augment: False \n",
      " Thread workers: 2\n",
      "2021-05-21 11:58:31,777 INFO No augmentation\n",
      "2021-05-21 11:58:31,790 INFO params of generator:\n",
      "2021-05-21 11:58:31,790 INFO [('MASKING_IMAGE', False), ('SINGLE_OUTPUT', False), ('MASKING_VALUES', [1, 2, 3]), ('DEBUG_MODE', False), ('SCALER', 'Standard'), ('AUGMENT', False), ('SHUFFLE', True), ('RESAMPLE', True), ('SPACING', [2, 2, 2]), ('SEED', 42), ('DIM', [64, 128, 128]), ('BATCHSIZE', 2), ('MASK_VALUES', [1, 2, 3]), ('N_CLASSES', 3), ('MAX_WORKERS', 2), ('T_SPACING', 55), ('PHASES', 5), ('HIST_MATCHING', False), ('IMG_INTERPOLATION', 1), ('MSK_INTERPOLATION', 0), ('AUGMENT_TEMP', False), ('RESAMPLE_T', True), ('ISACDC', False), ('METADATA_FILE', '/mnt/ssd/data/gcn/metadata/SAx_3D_dicomTags_phase')]\n"
     ]
    }
   ],
   "source": [
    "# initialise a generator with these files\n",
    "# logging.getLogger().setLevel(logging.INFO)\n",
    "from src.data.Generators import PhaseWindowGenerator\n",
    "config['AUGMENT'] = True\n",
    "config['RESAMPLE'] = True\n",
    "batch_generator = PhaseWindowGenerator(x_train_sax, x_train_sax, config=config)\n",
    "val_config = config.copy()\n",
    "val_config['AUGMENT'] = False\n",
    "val_config['AUGMENT_TEMP'] = False\n",
    "val_config['HIST_MATCHING'] = False\n",
    "validation_generator = PhaseWindowGenerator(x_val_sax, x_val_sax , config=val_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22a59e350d4dc98421b7ed456d4a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=52, description='batch', max=104), IntSlider(value=0, description='im', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise one batch\n",
    "@interact_manual\n",
    "def select_image_in_batch(batch = (0,len(batch_generator), 1), im = (0,BATCHSIZE- 1, 1), slice_n=(1,11), save=False, filepath='data/temp/', filename='temp_x.npy'):\n",
    "    \n",
    "    import time\n",
    "    global inputs, outputs\n",
    "    t1 = time.time()\n",
    "    inputs, outputs = batch_generator.__getitem__(batch)\n",
    "    if type(inputs)==list:inputs, outputs = inputs[0], outputs[0]\n",
    "    logging.info('Preprocessing took: {:0.3f}'.format(time.time() - t1))\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    print('selected batch : ' + str(batch))\n",
    "    \n",
    "    selected_input = inputs[im]\n",
    "    selected_output = outputs[im]\n",
    "    \n",
    "    list(map(lambda x: show_2D_or_3D(img=x[::slice_n], interpol='bilinear',dpi=100,f_size=(5,5)), selected_input))\n",
    "    plt.show()\n",
    "    info('-'*100)\n",
    "    list(map(lambda y: show_2D_or_3D(img=y[::slice_n], interpol='bilinear',dpi=100,f_size=(5,5)), selected_output))\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(selected_input.flatten(), bins=np.linspace(0.1,1,20))\n",
    "    plt.show()\n",
    "    if save:\n",
    "        ensure_dir(filepath)\n",
    "        np.save(os.path.join(filepath, filename), x[im])\n",
    "        logging.info('saved to {}'.format(os.path.join(filepath, filename)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7354dfe0a9349d9b9451812e23116e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=52, description='batch', max=104), IntSlider(value=0, description='im', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise one batch\n",
    "@interact_manual\n",
    "def select_image_in_batch(batch = (0,len(batch_generator), 1), im = (0,BATCHSIZE- 1, 1), slice_n=(1,11), save=False, filepath='data/temp/', filename='temp_x.npy'):\n",
    "    \n",
    "    import time\n",
    "    global inputs, outputs\n",
    "    t1 = time.time()\n",
    "    inputs, outputs = batch_generator.__getitem__(batch)\n",
    "    if type(inputs)==list:inputs, outputs = inputs[0], outputs[0]\n",
    "    logging.info('Preprocessing took: {:0.3f}'.format(time.time() - t1))\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    print('selected batch : ' + str(batch))\n",
    "    \n",
    "    selected_input = inputs[im]\n",
    "    selected_output = outputs[im]\n",
    "    \n",
    "    # plot the pre (t-1) and post (t+1) volume of each cardiac phase and the difference\n",
    "    _ = list(map(lambda x : show_two_timesteps(x),  zip(['ED#', 'MS#', 'ES#', 'PF#', 'MD#'],np.stack([selected_input, selected_output], axis=1))))\n",
    "\n",
    "    plt.hist(selected_input.flatten(), bins=np.linspace(0.1,1,20))\n",
    "    plt.show()\n",
    "    if save:\n",
    "        ensure_dir(filepath)\n",
    "        np.save(os.path.join(filepath, filename), x[im])\n",
    "        logging.info('saved to {}'.format(os.path.join(filepath, filename)))\n",
    "        \n",
    "def show_two_timesteps(x):\n",
    "    # x is a tuple of phase (string), ndarray with the following shape: 2,z,x,y --> first and second timestep of the motion window\n",
    "    p, x = x\n",
    "    inter, dpi, f_size = 'bilinear', 100, (5,5)\n",
    "    logging.info('Phase: {}'.format(p))\n",
    "    show_2D_or_3D(img=x[0], interpol=inter,dpi=dpi,f_size=f_size);plt.show()\n",
    "    show_2D_or_3D(img=x[1], interpol=inter,dpi=dpi,f_size=f_size);plt.show()\n",
    "    show_2D_or_3D(img=x[0] - x[1], interpol=None,dpi=dpi,f_size=f_size);plt.show()\n",
    "    logging.info(\"-\"*20)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using tensorflow, need to monkey patch\n",
      "tf.python.backend.slice overwritten by monkey patch\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:32,728 INFO Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 128, 128, 1)\n",
      "(None, 64, 128, 128, 1)\n",
      "WARNING:tensorflow:From /home/sven/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:33,853 WARNING From /home/sven/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "from src.models.Models import create_RegistrationModel\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "if tensorflow.distribute.has_strategy():\n",
    "    strategy = tensorflow.distribute.get_strategy()\n",
    "else:\n",
    "    # distribute the training with the \"mirrored data\"-paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tensorflow.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "with strategy.scope():\n",
    "    model = create_RegistrationModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simpleregister\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5, 64, 128,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_unstack (TensorFlow [(None, 64, 128, 128 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "unet (Functional)               (None, 64, 128, 128, 1604832     tf_op_layer_unstack[0][3]        \n",
      "                                                                 tf_op_layer_unstack[0][1]        \n",
      "                                                                 tf_op_layer_unstack[0][2]        \n",
      "                                                                 tf_op_layer_unstack[0][4]        \n",
      "                                                                 tf_op_layer_unstack[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "unet2flow (Conv3D)              (None, 64, 128, 128, 1299        unet[0][0]                       \n",
      "                                                                 unet[1][0]                       \n",
      "                                                                 unet[2][0]                       \n",
      "                                                                 unet[3][0]                       \n",
      "                                                                 unet[4][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "deformable_layer (SpatialTransf (None, 64, 128, 128, 0           tf_op_layer_unstack[0][0]        \n",
      "                                                                 unet2flow[4][0]                  \n",
      "                                                                 tf_op_layer_unstack[0][1]        \n",
      "                                                                 unet2flow[1][0]                  \n",
      "                                                                 tf_op_layer_unstack[0][2]        \n",
      "                                                                 unet2flow[2][0]                  \n",
      "                                                                 tf_op_layer_unstack[0][3]        \n",
      "                                                                 unet2flow[0][0]                  \n",
      "                                                                 tf_op_layer_unstack[0][4]        \n",
      "                                                                 unet2flow[3][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 5, 64, 128,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 5, 64, 128,  0           deformable_layer[0][0]           \n",
      "                                                                 deformable_layer[1][0]           \n",
      "                                                                 deformable_layer[2][0]           \n",
      "                                                                 deformable_layer[3][0]           \n",
      "                                                                 deformable_layer[4][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_1 (TensorFlow [(None, 5, 64, 128,  0           unet2flow[4][0]                  \n",
      "                                                                 unet2flow[1][0]                  \n",
      "                                                                 unet2flow[2][0]                  \n",
      "                                                                 unet2flow[0][0]                  \n",
      "                                                                 unet2flow[3][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,606,131\n",
      "Trainable params: 1,606,131\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.losses import mse\n",
    "from src.utils.Metrics import Grad, MSE_\n",
    "if 'strategy' in locals():\n",
    "    pass\n",
    "else:\n",
    "    # distribute the training with the \"mirrored data\"-paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tensorflow.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "with strategy.scope():\n",
    "\n",
    "    losses = [mse, Grad('l1').loss]\n",
    "    weights = [1,0.01]\n",
    "    model.compile(optimizer=tensorflow.keras.optimizers.Adam(), loss=losses, loss_weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/sven/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:36,569 WARNING From /home/sven/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:43,746 INFO batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:44,141 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:44,143 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:44,146 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:44,149 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:44,152 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:44,155 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:53,010 INFO batch_all_reduce: 36 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:53,227 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:53,229 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:53,232 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 11:58:53,234 INFO Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - ETA: 0s - loss: 0.1343 - tf_op_layer_stack_loss: 0.1324 - tf_op_layer_stack_1_loss: 0.1890\n",
      "Epoch 00001: loss improved from inf to 0.13433, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 224s 2s/step - loss: 0.1343 - tf_op_layer_stack_loss: 0.1324 - tf_op_layer_stack_1_loss: 0.1890 - val_loss: 0.1342 - val_tf_op_layer_stack_loss: 0.1341 - val_tf_op_layer_stack_1_loss: 0.0120 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1094 - tf_op_layer_stack_loss: 0.1090 - tf_op_layer_stack_1_loss: 0.0344\n",
      "Epoch 00002: loss improved from 0.13433 to 0.10937, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 233s 2s/step - loss: 0.1094 - tf_op_layer_stack_loss: 0.1090 - tf_op_layer_stack_1_loss: 0.0344 - val_loss: 0.1277 - val_tf_op_layer_stack_loss: 0.1275 - val_tf_op_layer_stack_1_loss: 0.0223 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1051 - tf_op_layer_stack_loss: 0.1046 - tf_op_layer_stack_1_loss: 0.0418\n",
      "Epoch 00003: loss improved from 0.10937 to 0.10506, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 233s 2s/step - loss: 0.1051 - tf_op_layer_stack_loss: 0.1046 - tf_op_layer_stack_1_loss: 0.0418 - val_loss: 0.1263 - val_tf_op_layer_stack_loss: 0.1260 - val_tf_op_layer_stack_1_loss: 0.0315 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1035 - tf_op_layer_stack_loss: 0.1031 - tf_op_layer_stack_1_loss: 0.0443\n",
      "Epoch 00004: loss improved from 0.10506 to 0.10351, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 234s 2s/step - loss: 0.1035 - tf_op_layer_stack_loss: 0.1031 - tf_op_layer_stack_1_loss: 0.0443 - val_loss: 0.1240 - val_tf_op_layer_stack_loss: 0.1237 - val_tf_op_layer_stack_1_loss: 0.0364 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1031 - tf_op_layer_stack_loss: 0.1027 - tf_op_layer_stack_1_loss: 0.0444\n",
      "Epoch 00005: loss improved from 0.10351 to 0.10315, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 238s 2s/step - loss: 0.1031 - tf_op_layer_stack_loss: 0.1027 - tf_op_layer_stack_1_loss: 0.0444 - val_loss: 0.1211 - val_tf_op_layer_stack_loss: 0.1208 - val_tf_op_layer_stack_1_loss: 0.0357 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1049 - tf_op_layer_stack_loss: 0.1044 - tf_op_layer_stack_1_loss: 0.0453\n",
      "Epoch 00006: loss did not improve from 0.10315\n",
      "104/104 [==============================] - 238s 2s/step - loss: 0.1049 - tf_op_layer_stack_loss: 0.1044 - tf_op_layer_stack_1_loss: 0.0453 - val_loss: 0.1203 - val_tf_op_layer_stack_loss: 0.1200 - val_tf_op_layer_stack_1_loss: 0.0290 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1054 - tf_op_layer_stack_loss: 0.1050 - tf_op_layer_stack_1_loss: 0.0460\n",
      "Epoch 00007: loss did not improve from 0.10315\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1054 - tf_op_layer_stack_loss: 0.1050 - tf_op_layer_stack_1_loss: 0.0460 - val_loss: 0.1202 - val_tf_op_layer_stack_loss: 0.1199 - val_tf_op_layer_stack_1_loss: 0.0368 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1040 - tf_op_layer_stack_loss: 0.1035 - tf_op_layer_stack_1_loss: 0.0457\n",
      "Epoch 00008: loss did not improve from 0.10315\n",
      "104/104 [==============================] - 235s 2s/step - loss: 0.1040 - tf_op_layer_stack_loss: 0.1035 - tf_op_layer_stack_1_loss: 0.0457 - val_loss: 0.1206 - val_tf_op_layer_stack_loss: 0.1203 - val_tf_op_layer_stack_1_loss: 0.0336 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1036 - tf_op_layer_stack_loss: 0.1031 - tf_op_layer_stack_1_loss: 0.0473\n",
      "Epoch 00009: loss did not improve from 0.10315\n",
      "104/104 [==============================] - 235s 2s/step - loss: 0.1036 - tf_op_layer_stack_loss: 0.1031 - tf_op_layer_stack_1_loss: 0.0473 - val_loss: 0.1199 - val_tf_op_layer_stack_loss: 0.1196 - val_tf_op_layer_stack_1_loss: 0.0309 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1024 - tf_op_layer_stack_loss: 0.1019 - tf_op_layer_stack_1_loss: 0.0458\n",
      "Epoch 00010: loss improved from 0.10315 to 0.10238, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 234s 2s/step - loss: 0.1024 - tf_op_layer_stack_loss: 0.1019 - tf_op_layer_stack_1_loss: 0.0458 - val_loss: 0.1175 - val_tf_op_layer_stack_loss: 0.1172 - val_tf_op_layer_stack_1_loss: 0.0370 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1017 - tf_op_layer_stack_loss: 0.1012 - tf_op_layer_stack_1_loss: 0.0468\n",
      "Epoch 00011: loss improved from 0.10238 to 0.10167, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 234s 2s/step - loss: 0.1017 - tf_op_layer_stack_loss: 0.1012 - tf_op_layer_stack_1_loss: 0.0468 - val_loss: 0.1209 - val_tf_op_layer_stack_loss: 0.1205 - val_tf_op_layer_stack_1_loss: 0.0411 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1012 - tf_op_layer_stack_loss: 0.1008 - tf_op_layer_stack_1_loss: 0.0473\n",
      "Epoch 00012: loss improved from 0.10167 to 0.10123, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1012 - tf_op_layer_stack_loss: 0.1008 - tf_op_layer_stack_1_loss: 0.0473 - val_loss: 0.1198 - val_tf_op_layer_stack_loss: 0.1194 - val_tf_op_layer_stack_1_loss: 0.0363 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1015 - tf_op_layer_stack_loss: 0.1010 - tf_op_layer_stack_1_loss: 0.0466\n",
      "Epoch 00013: loss did not improve from 0.10123\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1015 - tf_op_layer_stack_loss: 0.1010 - tf_op_layer_stack_1_loss: 0.0466 - val_loss: 0.1223 - val_tf_op_layer_stack_loss: 0.1219 - val_tf_op_layer_stack_1_loss: 0.0386 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1020 - tf_op_layer_stack_loss: 0.1015 - tf_op_layer_stack_1_loss: 0.0473\n",
      "Epoch 00014: loss did not improve from 0.10123\n",
      "104/104 [==============================] - 238s 2s/step - loss: 0.1020 - tf_op_layer_stack_loss: 0.1015 - tf_op_layer_stack_1_loss: 0.0473 - val_loss: 0.1270 - val_tf_op_layer_stack_loss: 0.1267 - val_tf_op_layer_stack_1_loss: 0.0326 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1038 - tf_op_layer_stack_loss: 0.1034 - tf_op_layer_stack_1_loss: 0.0473\n",
      "Epoch 00015: loss did not improve from 0.10123\n",
      "104/104 [==============================] - 235s 2s/step - loss: 0.1038 - tf_op_layer_stack_loss: 0.1034 - tf_op_layer_stack_1_loss: 0.0473 - val_loss: 0.1195 - val_tf_op_layer_stack_loss: 0.1192 - val_tf_op_layer_stack_1_loss: 0.0356 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1061 - tf_op_layer_stack_loss: 0.1056 - tf_op_layer_stack_1_loss: 0.0475\n",
      "Epoch 00016: loss did not improve from 0.10123\n",
      "104/104 [==============================] - 238s 2s/step - loss: 0.1061 - tf_op_layer_stack_loss: 0.1056 - tf_op_layer_stack_1_loss: 0.0475 - val_loss: 0.1201 - val_tf_op_layer_stack_loss: 0.1198 - val_tf_op_layer_stack_1_loss: 0.0371 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1073 - tf_op_layer_stack_loss: 0.1068 - tf_op_layer_stack_1_loss: 0.0479\n",
      "Epoch 00017: loss did not improve from 0.10123\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1073 - tf_op_layer_stack_loss: 0.1068 - tf_op_layer_stack_1_loss: 0.0479 - val_loss: 0.1252 - val_tf_op_layer_stack_loss: 0.1248 - val_tf_op_layer_stack_1_loss: 0.0343 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1028 - tf_op_layer_stack_loss: 0.1023 - tf_op_layer_stack_1_loss: 0.0480\n",
      "Epoch 00018: loss did not improve from 0.10123\n",
      "104/104 [==============================] - 235s 2s/step - loss: 0.1028 - tf_op_layer_stack_loss: 0.1023 - tf_op_layer_stack_1_loss: 0.0480 - val_loss: 0.1206 - val_tf_op_layer_stack_loss: 0.1202 - val_tf_op_layer_stack_1_loss: 0.0362 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0999 - tf_op_layer_stack_loss: 0.0995 - tf_op_layer_stack_1_loss: 0.0465\n",
      "Epoch 00019: loss improved from 0.10123 to 0.09993, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 237s 2s/step - loss: 0.0999 - tf_op_layer_stack_loss: 0.0995 - tf_op_layer_stack_1_loss: 0.0465 - val_loss: 0.1159 - val_tf_op_layer_stack_loss: 0.1154 - val_tf_op_layer_stack_1_loss: 0.0442 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1011 - tf_op_layer_stack_loss: 0.1006 - tf_op_layer_stack_1_loss: 0.0470\n",
      "Epoch 00020: loss did not improve from 0.09993\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1011 - tf_op_layer_stack_loss: 0.1006 - tf_op_layer_stack_1_loss: 0.0470 - val_loss: 0.1209 - val_tf_op_layer_stack_loss: 0.1205 - val_tf_op_layer_stack_1_loss: 0.0355 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1010 - tf_op_layer_stack_loss: 0.1006 - tf_op_layer_stack_1_loss: 0.0472\n",
      "Epoch 00021: loss did not improve from 0.09993\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1010 - tf_op_layer_stack_loss: 0.1006 - tf_op_layer_stack_1_loss: 0.0472 - val_loss: 0.1197 - val_tf_op_layer_stack_loss: 0.1194 - val_tf_op_layer_stack_1_loss: 0.0347 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1000 - tf_op_layer_stack_loss: 0.0995 - tf_op_layer_stack_1_loss: 0.0471\n",
      "Epoch 00022: loss did not improve from 0.09993\n",
      "104/104 [==============================] - 235s 2s/step - loss: 0.1000 - tf_op_layer_stack_loss: 0.0995 - tf_op_layer_stack_1_loss: 0.0471 - val_loss: 0.1136 - val_tf_op_layer_stack_loss: 0.1133 - val_tf_op_layer_stack_1_loss: 0.0367 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1003 - tf_op_layer_stack_loss: 0.0999 - tf_op_layer_stack_1_loss: 0.0470\n",
      "Epoch 00023: loss did not improve from 0.09993\n",
      "104/104 [==============================] - 235s 2s/step - loss: 0.1003 - tf_op_layer_stack_loss: 0.0999 - tf_op_layer_stack_1_loss: 0.0470 - val_loss: 0.1171 - val_tf_op_layer_stack_loss: 0.1167 - val_tf_op_layer_stack_1_loss: 0.0376 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1022 - tf_op_layer_stack_loss: 0.1017 - tf_op_layer_stack_1_loss: 0.0470\n",
      "Epoch 00024: loss did not improve from 0.09993\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "104/104 [==============================] - 240s 2s/step - loss: 0.1022 - tf_op_layer_stack_loss: 0.1017 - tf_op_layer_stack_1_loss: 0.0470 - val_loss: 0.1220 - val_tf_op_layer_stack_loss: 0.1217 - val_tf_op_layer_stack_1_loss: 0.0312 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1031 - tf_op_layer_stack_loss: 0.1026 - tf_op_layer_stack_1_loss: 0.0475\n",
      "Epoch 00025: loss did not improve from 0.09993\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1031 - tf_op_layer_stack_loss: 0.1026 - tf_op_layer_stack_1_loss: 0.0475 - val_loss: 0.1138 - val_tf_op_layer_stack_loss: 0.1134 - val_tf_op_layer_stack_1_loss: 0.0384 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0999 - tf_op_layer_stack_loss: 0.0994 - tf_op_layer_stack_1_loss: 0.0473\n",
      "Epoch 00026: loss improved from 0.09993 to 0.09985, saving model to exp/temp/windowmotion/2021-05-21_11_58/model/model.h5\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.0999 - tf_op_layer_stack_loss: 0.0994 - tf_op_layer_stack_1_loss: 0.0473 - val_loss: 0.1134 - val_tf_op_layer_stack_loss: 0.1129 - val_tf_op_layer_stack_1_loss: 0.0411 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1010 - tf_op_layer_stack_loss: 0.1006 - tf_op_layer_stack_1_loss: 0.0475\n",
      "Epoch 00027: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 237s 2s/step - loss: 0.1010 - tf_op_layer_stack_loss: 0.1006 - tf_op_layer_stack_1_loss: 0.0475 - val_loss: 0.1191 - val_tf_op_layer_stack_loss: 0.1187 - val_tf_op_layer_stack_1_loss: 0.0350 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1005 - tf_op_layer_stack_loss: 0.1000 - tf_op_layer_stack_1_loss: 0.0469\n",
      "Epoch 00028: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1005 - tf_op_layer_stack_loss: 0.1000 - tf_op_layer_stack_1_loss: 0.0469 - val_loss: 0.1147 - val_tf_op_layer_stack_loss: 0.1144 - val_tf_op_layer_stack_1_loss: 0.0380 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1016 - tf_op_layer_stack_loss: 0.1011 - tf_op_layer_stack_1_loss: 0.0471\n",
      "Epoch 00029: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 236s 2s/step - loss: 0.1016 - tf_op_layer_stack_loss: 0.1011 - tf_op_layer_stack_1_loss: 0.0471 - val_loss: 0.1201 - val_tf_op_layer_stack_loss: 0.1197 - val_tf_op_layer_stack_1_loss: 0.0354 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1008 - tf_op_layer_stack_loss: 0.1003 - tf_op_layer_stack_1_loss: 0.0475\n",
      "Epoch 00030: loss did not improve from 0.09985\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "104/104 [==============================] - 239s 2s/step - loss: 0.1008 - tf_op_layer_stack_loss: 0.1003 - tf_op_layer_stack_1_loss: 0.0475 - val_loss: 0.1149 - val_tf_op_layer_stack_loss: 0.1145 - val_tf_op_layer_stack_1_loss: 0.0395 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1006 - tf_op_layer_stack_loss: 0.1001 - tf_op_layer_stack_1_loss: 0.0474\n",
      "Epoch 00031: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 242s 2s/step - loss: 0.1006 - tf_op_layer_stack_loss: 0.1001 - tf_op_layer_stack_1_loss: 0.0474 - val_loss: 0.1176 - val_tf_op_layer_stack_loss: 0.1172 - val_tf_op_layer_stack_1_loss: 0.0393 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1013 - tf_op_layer_stack_loss: 0.1008 - tf_op_layer_stack_1_loss: 0.0476\n",
      "Epoch 00032: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 245s 2s/step - loss: 0.1013 - tf_op_layer_stack_loss: 0.1008 - tf_op_layer_stack_1_loss: 0.0476 - val_loss: 0.1139 - val_tf_op_layer_stack_loss: 0.1135 - val_tf_op_layer_stack_1_loss: 0.0366 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1003 - tf_op_layer_stack_loss: 0.0998 - tf_op_layer_stack_1_loss: 0.0472\n",
      "Epoch 00033: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 242s 2s/step - loss: 0.1003 - tf_op_layer_stack_loss: 0.0998 - tf_op_layer_stack_1_loss: 0.0472 - val_loss: 0.1138 - val_tf_op_layer_stack_loss: 0.1134 - val_tf_op_layer_stack_1_loss: 0.0396 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1017 - tf_op_layer_stack_loss: 0.1012 - tf_op_layer_stack_1_loss: 0.0476\n",
      "Epoch 00034: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 243s 2s/step - loss: 0.1017 - tf_op_layer_stack_loss: 0.1012 - tf_op_layer_stack_1_loss: 0.0476 - val_loss: 0.1179 - val_tf_op_layer_stack_loss: 0.1175 - val_tf_op_layer_stack_1_loss: 0.0392 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1016 - tf_op_layer_stack_loss: 0.1012 - tf_op_layer_stack_1_loss: 0.0476\n",
      "Epoch 00035: loss did not improve from 0.09985\n",
      "104/104 [==============================] - 242s 2s/step - loss: 0.1016 - tf_op_layer_stack_loss: 0.1012 - tf_op_layer_stack_1_loss: 0.0476 - val_loss: 0.1137 - val_tf_op_layer_stack_loss: 0.1133 - val_tf_op_layer_stack_1_loss: 0.0401 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      " 71/104 [===================>..........] - ETA: 1:04 - loss: 0.1038 - tf_op_layer_stack_loss: 0.1033 - tf_op_layer_stack_1_loss: 0.0485"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-efd7940c6fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasCallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m results = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.utils.KerasCallbacks import get_callbacks\n",
    "\n",
    "results = model.fit(\n",
    "    x=batch_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=100,\n",
    "    callbacks = get_callbacks(config, batch_generator,validation_generator),\n",
    "    initial_epoch=0,\n",
    "\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 64, 128, 128, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 64, 128, 128, 1)\n",
      "(2, 5, 64, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "transformed, flow = pred\n",
    "print(transformed.shape)\n",
    "print(flow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('reports/flow.npz',flow)\n",
    "np.save('reports/input.npz',inputs)\n",
    "np.save('reports/target.npz',outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as nifti\n",
    "\n",
    "- 5 x 3D input/output/flow\n",
    "- z x 2D+t input/output/flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-21 14:28:14,645 INFO exp/temp/windowmotion/2021-05-21_11_58/example_flows\n"
     ]
    }
   ],
   "source": [
    "def save_3d(nda, fname):\n",
    "    # save one flowfield\n",
    "    sitk_img = sitk.GetImageFromArray(nda, isVector=False)\n",
    "    sitk.WriteImage(sitk_img, fname)\n",
    "\n",
    "def save_all_3d_vols(inputs, outputs, flow, exp='example_flows'):\n",
    "    experiment_ = '{}/{}'.format(EXP_PATH, exp)\n",
    "    info(experiment_)\n",
    "    ensure_dir(experiment_)\n",
    "    flowname = os.path.join(experiment_, '_flow.nii')\n",
    "    firstfilename = os.path.join(experiment_, '_cmr.nii')\n",
    "    secondfilename = os.path.join(experiment_, '_targetcmr.nii')\n",
    "    \n",
    "    # invert the axis\n",
    "    flow = np.einsum('tzyxc->cxyzt', flow)\n",
    "    inputs = np.einsum('tzyxc->cxyzt', inputs)\n",
    "    outputs = np.einsum('tzyxc->cxyzt', outputs)\n",
    "    \n",
    "    _ = [save_3d(flow[...,t], flowname.replace('.nii', '_{}_.nii'.format(t))) for t in range(flow.shape[-1])]\n",
    "    _ = [save_3d(inputs[...,t], firstfilename.replace('.nii', '_{}_.nii'.format(t))) for t in range(inputs.shape[-1])]\n",
    "    _ = [save_3d(outputs[...,t], secondfilename.replace('.nii', '_{}_.nii'.format(t))) for t in range(outputs.shape[-1])]\n",
    "    \n",
    "    _ = [save_3d(flow[...,t,:], flowname.replace('.nii', '_sequence_{}_.nii'.format(t))) for t in range(flow.shape[-2])]\n",
    "    _ = [save_3d(inputs[...,t,:], firstfilename.replace('.nii', '_sequence_{}_.nii'.format(t))) for t in range(inputs.shape[-2])]\n",
    "    _ = [save_3d(outputs[...,t,:], secondfilename.replace('.nii', '_sequence_{}_.nii'.format(t))) for t in range(outputs.shape[-2])]\n",
    "    \n",
    "save_all_3d_vols(inputs[0], outputs[0], flow[0], 'example_flow_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save one example 3D flowfield as nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ = '{}/{}'.format(EXP_PATH, 'example_flows')\n",
    "ensure_dir(experiment_)\n",
    "flowname = os.path.join(experiment_, '_flow.nii')\n",
    "firstfilename = os.path.join(experiment_, '_cmr.nii')\n",
    "secondfilename = os.path.join(experiment_, '_targetcmr.nii')\n",
    "t = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 128, 128, 3)\n",
      "(32, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "flow_single = flow[0]\n",
    "print(flow_single.shape)\n",
    "flow_single = np.einsum('tzyxc->cxyzt', flow_single)\n",
    "flow_single = flow_single[...,t]\n",
    "flow_sitk = sitk.GetImageFromArray(flow_single, isVector=False)\n",
    "print(flow_sitk.GetSize())\n",
    "sitk.WriteImage(flow_sitk, flowname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 128, 128, 1)\n",
      "(32, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# save the first time step\n",
    "inputs_single = inputs[0]\n",
    "print(inputs_single.shape)\n",
    "inputs_single = np.einsum('tzyxc->cxyzt', inputs_single)\n",
    "inputs_single = inputs_single[...,t]\n",
    "input_sitk = sitk.GetImageFromArray(inputs_single, isVector=False)\n",
    "print(input_sitk.GetSize())\n",
    "sitk.WriteImage(input_sitk, firstfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 128, 128, 1)\n",
      "(32, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# save the second time step\n",
    "outputs_single = outputs[0]\n",
    "print(outputs_single.shape)\n",
    "outputs_single = np.einsum('tzyxc->cxyzt', outputs_single)\n",
    "outputs_single = outputs_single[...,t]\n",
    "output_sitk = sitk.GetImageFromArray(outputs_single, isVector=False)\n",
    "print(output_sitk.GetSize())\n",
    "sitk.WriteImage(output_sitk, secondfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save one example 2D+T flowfield as nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporalflowname = flowname.replace('flow.nii', 'tempflow.nii')\n",
    "temporalfirstfilename = firstfilename.replace('cmr.nii', 'tempcmr.nii')\n",
    "temporalsecondfilename = secondfilename.replace('targetcmr.nii', 'temptargetcmr.nii')\n",
    "slice_ = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 128, 128, 3)\n",
      "(3, 128, 128, 32, 5)\n",
      "(3, 128, 128, 5)\n",
      "(5, 128, 128, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# save one 2D+t flowfield\n",
    "flow_single = flow[0]\n",
    "print(flow_single.shape)\n",
    "flow_single = np.einsum('tzyxc->cxyzt', flow_single)\n",
    "print(flow_single.shape)\n",
    "flow_single = flow_single[:,:,:,slice_,:]\n",
    "print(flow_single.shape)\n",
    "flow_sitk = sitk.GetImageFromArray(flow_single, isVector=False)\n",
    "print(flow_sitk.GetSize())\n",
    "print(flow_sitk.GetNumberOfComponentsPerPixel())\n",
    "sitk.WriteImage(flow_sitk, temporalflowname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save 4D flow - not working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 128, 128, 3)\n",
      "(3, 128, 128, 30, 5)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# save the second time step\n",
    "temp = flow[0]\n",
    "print(temp.shape)\n",
    "#temp = np.einsum('tzyxc->ctzyx', temp)\n",
    "#temp = [temp[:,:,:,i,:] for i in range(outputs_single.shape[-2])]\n",
    "temp = sitk.JoinSeries([sitk.GetImageFromArray(img, isVector=False) for img in temp])\n",
    "print(temp.GetSize())\n",
    "print(temp.GetNumberOfComponentsPerPixel())\n",
    "sitk.WriteImage(temp, os.path.join(experiment_, '4dflow.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 128, 128, 3)\n",
      "(5, 128, 128, 3)\n",
      "(3, 128, 128, 5)\n",
      "(3, 128, 128, 10)\n",
      "(128, 128, 3)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# stack two slices - tests\n",
    "flow_single = flow[0]\n",
    "print(flow_single.shape)\n",
    "flow_single = flow_single[:,slice_,:,:,:]\n",
    "print(flow_single.shape)\n",
    "flow_single = np.einsum('tyxc->cxyt', flow_single)\n",
    "print(flow_single.shape)\n",
    "flow_single = np.concatenate([flow_single,flow_single], axis=-1)\n",
    "print(flow_single.shape)\n",
    "flow_sitk = sitk.GetImageFromArray(flow_single, isVector=True)\n",
    "print(flow_sitk.GetSize())\n",
    "print(flow_sitk.GetNumberOfComponentsPerPixel())\n",
    "sitk.WriteImage(flow_sitk, temporalflowname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 128, 128, 3)\n",
      "(5, 1, 128, 128, 3)\n",
      "(3, 128, 128, 5)\n",
      "(3, 128, 128, 150)\n",
      "(150, 128, 128, 3)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# stack z x 2D+T flows - works\n",
    "flow_single = flow[0]\n",
    "print(flow_single.shape)\n",
    "flows = np.split(flow_single,indices_or_sections=flow_single.shape[1], axis=1)\n",
    "print(flows[0].shape)\n",
    "flows = [np.einsum('tyxc->cxyt', np.squeeze(f)) for f in flows]\n",
    "print(flows[0].shape)\n",
    "flows = np.concatenate(flows, axis=-1)\n",
    "print(flows.shape)\n",
    "flow_sitk = sitk.GetImageFromArray(flows, isVector=False)\n",
    "print(flow_sitk.GetSize())\n",
    "print(flow_sitk.GetNumberOfComponentsPerPixel())\n",
    "sitk.WriteImage(flow_sitk, experiment_+'/stacked_2d+t.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 128, 128)\n",
      "(128, 128, 150)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# stack z x 2D+T slices - tests\n",
    "flow_single = np.squeeze(inputs[0])\n",
    "print(flow_single.shape)\n",
    "flow_single = np.reshape(flow_single,(150,128,128))\n",
    "flow_sitk = sitk.GetImageFromArray(flow_single, isVector=False)\n",
    "print(flow_sitk.GetSize())\n",
    "print(flow_sitk.GetNumberOfComponentsPerPixel())\n",
    "sitk.WriteImage(flow_sitk, experiment_+'/stacked_1st_cmr.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'new_DisplacementFieldTransform'.\n  Possible C/C++ prototypes are:\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(unsigned int)\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(itk::simple::Image &)\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(itk::simple::DisplacementFieldTransform const &)\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(itk::simple::Transform const &)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-73b37a9acd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mflow_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tzyxc->cxyzt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mflows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisplacementFieldTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mflow_sitk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJoinSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_sitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-73b37a9acd07>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mflow_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tzyxc->cxyzt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mflows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisplacementFieldTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mflow_sitk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJoinSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_sitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dcmr/lib/python3.8/site-packages/SimpleITK/SimpleITK.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   4433\u001b[0m         \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDisplacementFieldTransform\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransform\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDisplacementFieldTransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4434\u001b[0m         \"\"\"\n\u001b[0;32m-> 4435\u001b[0;31m         \u001b[0m_SimpleITK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisplacementFieldTransform_swiginit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SimpleITK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_DisplacementFieldTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mGetName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'new_DisplacementFieldTransform'.\n  Possible C/C++ prototypes are:\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(unsigned int)\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(itk::simple::Image &)\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(itk::simple::DisplacementFieldTransform const &)\n    itk::simple::DisplacementFieldTransform::DisplacementFieldTransform(itk::simple::Transform const &)\n"
     ]
    }
   ],
   "source": [
    "# save one 2D+t flowfield\n",
    "flow_single = flow[0]\n",
    "flow_single = np.einsum('tzyxc->cxyzt', flow_single)\n",
    "flows = np.split(flow_single, indices_or_sections=5, axis=-1)\n",
    "flows = [sitk.DisplacementFieldTransform(f) for f in flows]\n",
    "flow_sitk = sitk.JoinSeries(flows)\n",
    "print(flow_sitk.GetSize())\n",
    "sitk.WriteTransform(flow_sitk, 'reports/examplefullflow_4d.nrrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30, 128, 128, 3)\n",
      "(5, 30, 128, 128, 3)\n",
      "(5, 30, 128, 128, 3)\n",
      "(3, 128, 128, 30, 5)\n"
     ]
    }
   ],
   "source": [
    "# save one flowfield\n",
    "flow_single = flow[0]\n",
    "print(flow_single.shape)\n",
    "#flow_single = np.einsum('tzyxc->ctzyx', flow_single)\n",
    "print(flow_single.shape)\n",
    "#flow_single = flow_single[...,t]\n",
    "\n",
    "#flow_single = np.reshape(flow_single, newshape=(15,30,128,128), order='F')\n",
    "print(flow_single.shape)\n",
    "flow_sitk = sitk.GetImageFromArray(flow_single, isVector=False)\n",
    "print(flow_sitk.GetSize())\n",
    "sitk.WriteImage(flow_sitk, flowname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_sitk = sitk.GetImageFromArray(flow[0], isVector=False)\n",
    "sitk.WriteImage(flow_sitk, 'reports/example2flow.ni.gz')\n",
    "\n",
    "\"\"\"inputs_sitk = sitk.GetImageFromArray(inputs)\n",
    "outputs_sitk = sitk.GetImageFromArray(outputs)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='t', max=5), IntSlider(value=2, description='slice_z', ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.visualization.Visualize import create_quiver_plot\n",
    "\n",
    "@interact\n",
    "def show_pred(t=(0,5), slice_z=(0,5), debug_=False):\n",
    "    info('input')\n",
    "    if debug_:list(map(lambda x: show_2D_or_3D(img=x[::slice_z], interpol='bilinear',dpi=100,f_size=(5,5)), inputs[0][t:t+1]));plt.show()\n",
    "    info('target')\n",
    "    if debug_:list(map(lambda x: show_2D_or_3D(img=x[::slice_z], interpol='bilinear',dpi=100,f_size=(5,5)), outputs[0][t:t+1]));plt.show()\n",
    "    diff = inputs - outputs\n",
    "    info('diff input')\n",
    "    if debug_:list(map(lambda x: show_2D_or_3D(img=x[::slice_z], interpol='bilinear',dpi=100,f_size=(5,5)), diff[0][t:t+1]));plt.show()\n",
    "    info('pred')\n",
    "    if debug_:list(map(lambda x: show_2D_or_3D(img=x[::slice_z], interpol='bilinear',dpi=100,f_size=(5,5)), transformed[0][t:t+1]));plt.show()\n",
    "    info('diff pred')\n",
    "    diff = transformed - outputs\n",
    "    if debug_:list(map(lambda x: show_2D_or_3D(img=x[::slice_z], interpol='bilinear',dpi=100,f_size=(5,5)), diff[0][t:t+1]));plt.show()\n",
    "    if debug_:fig,axes = plt.subplots(1,flow.shape[2]//slice_z, figsize=(24,2))\n",
    "    if debug_:list(map(lambda x: create_quiver_plot(flowfield_2d=x[1],indexing='ij', scale=0.5, N=1,ax=x[0]), zip(axes,flow[0][t][::slice_z]))); plt.show() # third timestep\n",
    "    list(map(lambda x: show_2D_or_3D(mask=x[::slice_z], interpol='bilinear',dpi=100,f_size=(5,5)) and plt.show(), flow[0][t:t+1]))\n",
    "    plt.show()\n",
    "    if debug_:plt.hist(flow[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcmr",
   "language": "python",
   "name": "dcmr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
