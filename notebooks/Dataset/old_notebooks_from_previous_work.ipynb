{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search for root_dir and set working directory\n",
      "Working directory set to: /mnt/ssd/git/dynamic-cmr-models\n",
      "['/gpu:0', '/gpu:1']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CycleMotionDataGenerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-480c35f30334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_2D_or_3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_img_msk_files_from_split_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_acdc_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_train_data_from_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_trainings_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCycleMotionDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasCallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss_and_metrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CycleMotionDataGenerator'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------define logging and working directory\n",
    "from ProjectRoot import change_wd_to_project_root\n",
    "change_wd_to_project_root()\n",
    "from src.utils.Notebook_imports import *\n",
    "from src.utils.Tensorflow_helper import choose_gpu_by_id\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import cv2\n",
    "# ------------------------------------------define GPU id/s to use\n",
    "GPU_IDS = '0,1'\n",
    "GPUS = choose_gpu_by_id(GPU_IDS)\n",
    "print(GPUS)\n",
    "# ------------------------------------------jupyter magic config\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# ------------------------------------------ import helpers\n",
    "from src.utils.Utils_io import Console_and_file_logger, init_config\n",
    "from src.visualization.Visualize import show_2D_or_3D\n",
    "from src.data.Dataset import get_img_msk_files_from_split_dir, load_acdc_files, get_train_data_from_df, get_trainings_files\n",
    "from src.data.Generators import DataGenerator, CycleMotionDataGenerator\n",
    "from src.utils.KerasCallbacks import get_callbacks\n",
    "import src.utils.Loss_and_metrics as metr\n",
    "import src.models.SpatialTransformer as st\n",
    "from src.models.SpatialTransformer import create_affine_cycle_transformer_model\n",
    "from src.models.ModelUtils import load_pretrained_model\n",
    "# ------------------------------------------path and project params\n",
    "ARCHITECTURE = '3D' # 2D\n",
    "DATASET = 'GCN'  # 'acdc' # or 'gcn' or different versions such as gcn_01/02\n",
    "FOLD = 0 # CV fold 0-3\n",
    "EXP_NAME = 'ax_sax/trained_on_original_ax_sax_pairs_temp/' # Define an experiment name, could have subfolder conventions\n",
    "EXPERIMENT = '{}/{}'.format(ARCHITECTURE, EXP_NAME) # Uniform path names, separation of concerns\n",
    "timestemp = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H_%M\")) # ad a timestep to each project to make repeated experiments unique\n",
    "\n",
    "# This noebook expects the path to the following folders\n",
    "# each folder contains an tuple of 3D nrrd_image, nrrd_masks\n",
    "# any-path/\n",
    "#    - AX_3D(anyname_img.nrrd and anyname_msk.nrrd)\n",
    "#    - AX_to_SAX_3D\n",
    "#    - SAX_3D\n",
    "#    - SAX_to_AX_3D\n",
    "DATA_PATH_AX = '/mnt/ssd/data/gcn/ax_sax_from_flo/ax3d/' # path to AX 3D files\n",
    "DATA_PATH_AX2SAX = '/mnt/ssd/data/gcn/ax_sax_from_flo/sax3d/' # path to transformed AX 3D files (target of AX)\n",
    "\n",
    "DATA_PATH_SAX = '/mnt/ssd/data/gcn/ax_sax_from_flo/sax3d/' # path to SAX 3D files\n",
    "DATA_PATH_SAX2AX = '/mnt/ssd/data/gcn/ax_sax_from_flo/ax3d/' # path to transformed SAX 3D files (target of SAX)\n",
    "\n",
    "DF_PATH = '/mnt/ssd/data/gcn/gcn_05_2020_ax_sax_86/folds.csv' # path to folds dataframe\n",
    "\n",
    "MODEL_PATH = os.path.join('models', EXPERIMENT, timestemp)\n",
    "TENSORBOARD_LOG_DIR = os.path.join('reports/tensorboard_logs', EXPERIMENT,timestemp)\n",
    "CONFIG_PATH = os.path.join('reports/configs/',EXPERIMENT,timestemp)\n",
    "HISTORY_PATH = os.path.join('reports/history/',EXPERIMENT,timestemp)\n",
    "\n",
    "# ------------------------------------------static model, loss and generator hyperparameters\n",
    "DIM = [40, 64, 64] # network input params for spacing of 3, (z,y,x)\n",
    "DEPTH = 4 # number of down-/upsampling blocks\n",
    "FILTERS = 16 # initial number of filters, will be doubled after each downsampling block\n",
    "SPACING = [6, 6, 6] # if resample, resample to this spacing, (z,y,x)\n",
    "M_POOL = [2, 2, 2]# size of max-pooling used for downsampling and upsampling\n",
    "F_SIZE = [3, 3, 3] # conv filter size\n",
    "IMG_CHANNELS = 1 # Currently our model needs that image channel\n",
    "MASK_VALUES = [1, 2, 3]  #channel order: Background, RV, MYO, LV\n",
    "MASK_CLASSES = len(MASK_VALUES) # no of labels\n",
    "BORDER_MODE = cv2.BORDER_REFLECT_101 # border mode for the data generation\n",
    "IMG_INTERPOLATION = cv2.INTER_LINEAR # image interpolation in the genarator\n",
    "MSK_INTERPOLATION = cv2.INTER_NEAREST # mask interpolation in the generator\n",
    "AUGMENT = False # Not implemented for the AX2SAX case\n",
    "SHUFFLE = True\n",
    "AUGMENT_GRID = False # Not implemented for the AX2SAX case\n",
    "RESAMPLE = True\n",
    "SCALER = 'MinMax' # MinMax Standard or Robust\n",
    "\n",
    "AX_LOSS_WEIGHT = 10.0 # weighting factor of the ax2sax loss\n",
    "WEIGHT_MSE_INPLANE = True # turn inplane weighting on/off\n",
    "MASK_SMALLER_THAN_THRESHOLD = 0.001 # define the threshold for masking the ax2sax/sax2ax MSE loss, areas with smaller values, will be masked out\n",
    "\n",
    "SAX_LOSS_WEIGHT = 10.0 # weighting factor of the sax2ax loss\n",
    "CYCLE_LOSS = True # turn this loss on/off\n",
    "\n",
    "FOCUS_LOSS_WEIGHT = 1.0 # weighting of the focus loss\n",
    "FOCUS_LOSS = True # turn this loss on/off\n",
    "USE_SAX2AX_PROB = False # apply the focus loss on AX2SAX_mask predictions, or on AX2SAX2AX_mask (back-transformed) predictions\n",
    "MIN_UNET_PROBABILITY = 0.9 # threshold to count only prediction greater than this value for the focus loss\n",
    "\n",
    "# ------------------------------------------individual training params\n",
    "GENERATOR_WORKER = 2 # number of parallel workers in our generator. if not set, use batchsize, numbers greater than batchsize does not make any sense\n",
    "SEED = 42 # define a seed for the generator shuffle\n",
    "BATCHSIZE = 2 # 32, 64, 24, 16, 1 for 3spacing 3,3,3 use: 2\n",
    "INITIAL_EPOCH = 0 # change this to continue training\n",
    "EPOCHS = 300 # define a maximum numbers of epochs\n",
    "EPOCHS_BETWEEN_CHECKPOINTS = 5\n",
    "MONITOR_FUNCTION = 'val_loss'\n",
    "MONITOR_MODE = 'min'\n",
    "SAVE_MODEL_FUNCTION = 'val_loss'\n",
    "SAVE_MODEL_MODE = 'min'\n",
    "MODEL_PATIENCE = 20\n",
    "BN_FIRST = False # decide if batch normalisation between conv and activation or afterwards\n",
    "BATCH_NORMALISATION = True # apply BN or not\n",
    "USE_UPSAMPLE = True # otherwise use transpose for upsampling\n",
    "PAD = 'same' # padding strategy of the conv layers\n",
    "KERNEL_INIT = 'he_normal' # conv weight initialisation\n",
    "OPTIMIZER = 'adam' # Adam, Adagrad, RMSprop, Adadelta,  # https://keras.io/optimizers/\n",
    "ACTIVATION = 'elu' # tf.keras.layers.LeakyReLU(), relu or any other non linear activation function\n",
    "LEARNING_RATE = 1e-4 # start with a huge lr to converge fast\n",
    "DECAY_FACTOR = 0.3 # Define a learning rate decay for the ReduceLROnPlateau callback\n",
    "MIN_LR = 1e-10 # minimal lr, smaller lr does not improve the model\n",
    "DROPOUT_min = 0.3 # lower dropout at the shallow layers\n",
    "DROPOUT_max = 0.5 # higher dropout at the deep layers\n",
    "\n",
    "# ------------------------------------------these metrics and loss function are meant if you continue training of the U-Net\n",
    "metrics = [\n",
    "    metr.dice_coef_labels,\n",
    "    metr.dice_coef_myo,\n",
    "    metr.dice_coef_lv,\n",
    "    metr.dice_coef_rv\n",
    "]\n",
    "LOSS_FUNCTION = metr.bce_dice_loss\n",
    "\n",
    "# Create a logger instance with the following setup: info or debug to console and file and error logs to a separate file\n",
    "# Define a config for param injection,\n",
    "# save a serialized version to load the experiment for prediction/evaluation, \n",
    "# make sure all paths exist\n",
    "Console_and_file_logger(EXPERIMENT, logging.INFO)\n",
    "config = init_config(config=locals(), save=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Tensorflow setup and available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "logging.info('Is built with tensorflow: {}'.format(tf.test.is_built_with_cuda()))\n",
    "logging.info('Visible devices:\\n{}'.format(tf.config.list_physical_devices()))\n",
    "logging.info('Local devices: \\n {}'.format(device_lib.list_local_devices()))\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "logging.info('Compute dtype: %s' % policy.compute_dtype)\n",
    "logging.info('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trainings and validation files for the choosen fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 15:54:59,000 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/ax3d/\n",
      "2020-12-23 15:54:59,001 INFO Patients train: 64\n",
      "2020-12-23 15:54:59,007 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-23 15:54:59,008 INFO AX train CMR: 120, AX train masks: 120\n",
      "2020-12-23 15:54:59,008 INFO AX val CMR: 42, AX val masks: 42\n",
      "2020-12-23 15:54:59,012 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/sax3d/\n",
      "2020-12-23 15:54:59,012 INFO Patients train: 64\n",
      "2020-12-23 15:54:59,018 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-23 15:54:59,019 INFO AX2SAX train CMR: 120, AX2SAX train masks: 120\n",
      "2020-12-23 15:54:59,020 INFO AX2SAX val CMR: 42, AX2SAX val masks: 42\n",
      "2020-12-23 15:54:59,024 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/sax3d/\n",
      "2020-12-23 15:54:59,024 INFO Patients train: 64\n",
      "2020-12-23 15:54:59,030 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-23 15:54:59,031 INFO SAX train CMR: 120, SAX train masks: 120\n",
      "2020-12-23 15:54:59,031 INFO SAX val CMR: 42, SAX val masks: 42\n",
      "2020-12-23 15:54:59,035 INFO Found 162 images/masks in /mnt/ssd/data/gcn/ax_sax_from_flo/ax3d/\n",
      "2020-12-23 15:54:59,035 INFO Patients train: 64\n",
      "2020-12-23 15:54:59,041 INFO Selected 120 of 162 files with 64 of 86 patients for training fold 0\n",
      "2020-12-23 15:54:59,042 INFO SAX2AX train CMR: 120, SAX2AX train masks: 120\n",
      "2020-12-23 15:54:59,043 INFO SAX2AX val CMR: 42, SAX2AX val masks: 42\n"
     ]
    }
   ],
   "source": [
    "# Load AX volumes\n",
    "x_train_ax, y_train_ax, x_val_ax, y_val_ax =  get_trainings_files(data_path=DATA_PATH_AX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('AX train CMR: {}, AX train masks: {}'.format(len(x_train_ax), len(y_train_ax)))\n",
    "logging.info('AX val CMR: {}, AX val masks: {}'.format(len(x_val_ax), len(y_val_ax)))\n",
    "\n",
    "# load AX2SAX volumes\n",
    "x_train_ax2sax, y_train_ax2sax, x_val_ax2sax, y_val_ax2sax =  get_trainings_files(data_path=DATA_PATH_AX2SAX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('AX2SAX train CMR: {}, AX2SAX train masks: {}'.format(len(x_train_ax2sax), len(y_train_ax2sax)))\n",
    "logging.info('AX2SAX val CMR: {}, AX2SAX val masks: {}'.format(len(x_val_ax2sax), len(y_val_ax2sax)))\n",
    "\n",
    "# Load SAX volumes\n",
    "x_train_sax, y_train_sax, x_val_sax, y_val_sax =  get_trainings_files(data_path=DATA_PATH_SAX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('SAX train CMR: {}, SAX train masks: {}'.format(len(x_train_sax), len(y_train_sax)))\n",
    "logging.info('SAX val CMR: {}, SAX val masks: {}'.format(len(x_val_sax), len(y_val_sax)))\n",
    "\n",
    "# load SAX2AX volumes\n",
    "x_train_sax2ax, y_train_sax2ax, x_val_sax2ax, y_val_sax2ax =  get_trainings_files(data_path=DATA_PATH_SAX2AX,path_to_folds_df=DF_PATH, fold=FOLD)\n",
    "logging.info('SAX2AX train CMR: {}, SAX2AX train masks: {}'.format(len(x_train_sax2ax), len(y_train_sax2ax)))\n",
    "logging.info('SAX2AX val CMR: {}, SAX2AX val masks: {}'.format(len(x_val_sax2ax), len(y_val_sax2ax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# filter files by name, debugging purpose\n",
    "search_str = '2cvu'.lower()\n",
    "x_val_ax = [x for x in x_val_ax if search_str in x.lower()]\n",
    "x_val_sax = [x for x in x_val_sax if search_str in x.lower()]\n",
    "x_val_ax2sax = [x for x in x_val_ax2sax if search_str in x.lower()]\n",
    "x_val_sax2ax = [x for x in x_val_sax2ax if search_str in x.lower()]\n",
    "print(len(x_val_ax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 15:55:00,184 INFO Create DataGenerator\n",
      "2020-12-23 15:55:00,185 INFO generator in debug mode = False\n",
      "2020-12-23 15:55:00,185 INFO Datagenerator created with: \n",
      " shape: [40, 64, 64]\n",
      " spacing: [6, 6, 6]\n",
      " batchsize: 2\n",
      " Scaler: MinMax\n",
      " Images: 120 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-23 15:55:00,185 INFO No augmentation\n",
      "2020-12-23 15:55:00,186 INFO Create DataGenerator\n",
      "2020-12-23 15:55:00,186 INFO generator in debug mode = False\n",
      "2020-12-23 15:55:00,187 INFO Datagenerator created with: \n",
      " shape: [40, 64, 64]\n",
      " spacing: [6, 6, 6]\n",
      " batchsize: 2\n",
      " Scaler: MinMax\n",
      " Images: 2 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-23 15:55:00,187 INFO No augmentation\n"
     ]
    }
   ],
   "source": [
    "# create two generators, one for the training files, one for the validation files\n",
    "batch_generator = CycleMotionDataGenerator(x=x_train_ax, y=x_train_ax2sax, x2=x_train_sax, y2=x_train_sax2ax, config=config)\n",
    "valid_config = config.copy()\n",
    "valid_config['AUGMENT_GRID'] = False\n",
    "valid_config['AUGMENT'] = False\n",
    "valid_generator = CycleMotionDataGenerator(x=x_val_ax, y=x_val_ax2sax, x2=x_val_sax, y2=x_val_sax2ax, config=valid_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd5544e991a4896970281365e5daf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='batch', max=1), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select batch generator output\n",
    "x = ''\n",
    "y = ''\n",
    "@interact\n",
    "def select_batch(batch = (0,len(valid_generator), 1)):\n",
    "    global x, y, x2, y2\n",
    "    input_ , output_ = valid_generator.__getitem__(batch)\n",
    "    x = input_[0]\n",
    "    y = output_[0]\n",
    "    x2 = input_[1]\n",
    "    y2 = output_[1]\n",
    "    logging.info('input elements: {}'.format(len(input_)))\n",
    "    logging.info('output elements: {}'.format(len(output_)))\n",
    "    logging.info(x.shape)\n",
    "    logging.info(y.shape)\n",
    "    logging.info(x2.shape)\n",
    "    logging.info(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936e632e8ba24e9981202969b6aaa17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='im', max=1), IntSlider(value=3, description='slice_by', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def select_image_in_batch(im = (0,x.shape[0]- 1, 1),slice_by=(1,6)):\n",
    "    \n",
    "    # define a different logging level to make the generator steps visible\n",
    "    #logging.getLogger().setLevel(logging.DEBUG)\n",
    "    temp_dir = 'reports/figures/temp/'\n",
    "    ensure_dir(temp_dir)\n",
    "\n",
    "    logging.info('AX: {}'.format(x[im].shape))\n",
    "    show_2D_or_3D(x[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'ax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('AXtoSAX: {}'.format(y[im].shape))\n",
    "    show_2D_or_3D(y[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'ax2sax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('SAX: {}'.format(x2[im].shape))\n",
    "    show_2D_or_3D(x2[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'sax.pdf'))\n",
    "    plt.show()\n",
    "    logging.info('SAXtoAX: {}'.format(y2[im].shape))\n",
    "    show_2D_or_3D(y2[im][...,0][::slice_by])\n",
    "    plt.savefig(os.path.join(temp_dir,'sax2ax.pdf'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-19 11:19:13,539 INFO Load model from Experiment: 2D/gcn_05_2020_sax_excl_ax_patients_baseline/lower_clip\n",
      "2020-12-19 11:19:13,539 INFO load model with keras api\n",
      "2020-12-19 11:19:15,941 INFO Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "2020-12-19 11:19:15,942 INFO Keras API failed, use json repr. load model from: /mnt/ssd/git/cardio/models/2D/gcn_05_2020_sax_excl_ax_patients_baseline/lower_clip/2020-12-18_20_03/model.json .\n",
      "2020-12-19 11:19:15,943 INFO loading model description\n",
      "2020-12-19 11:19:16,895 INFO loading model weights\n",
      "2020-12-19 11:19:17,065 INFO model /mnt/ssd/git/cardio/models/2D/gcn_05_2020_sax_excl_ax_patients_baseline/lower_clip/2020-12-18_20_03/model.json loaded\n"
     ]
    }
   ],
   "source": [
    "# load a pretrained 2D unet\n",
    "\"\"\"\n",
    "load past config for model training \n",
    "\"\"\"\n",
    "if 'config_chooser' in locals():\n",
    "    config_file  = config_chooser.selected\n",
    "else:\n",
    "    #config_file = '/mnt/ssd/git/3d-mri-domain-adaption/reports/configs/2D/gcn_and_acdc_excl_ax/config.json' # config for TMI paper\n",
    "    # config_file = '/mnt/ssd/git/cardio/reports/configs/2D/gcn_05_2020_sax_excl_ax_patients/2020-11-20_17_24/config.json' # retrained with downsampling\n",
    "     # retrained with downsampling and lower clipping\n",
    "    config_file = '/mnt/ssd/git/cardio/reports/configs/2D/gcn_05_2020_sax_excl_ax_patients_baseline/lower_clip/2020-12-18_20_03/config.json' #\n",
    "    \n",
    "\n",
    "# load config with all params into global namespace\n",
    "with open(config_file, encoding='utf-8') as data_file:\n",
    "    config_temp = json.loads(data_file.read())\n",
    "logging.info('Load model from Experiment: {}'.format(config_temp['EXPERIMENT']))\n",
    "\n",
    "if 'strategy' not in globals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "with strategy.scope():\n",
    "    globals()['unet'] = load_pretrained_model(config_temp, metrics, comp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-19 11:19:22,697 INFO unet given, use it to max probability\n",
      "2020-12-19 11:19:32,082 INFO adding ax2sax MSE loss with a weighting of 10.0\n",
      "2020-12-19 11:19:32,082 INFO adding cycle loss with a weighting of 10.0\n",
      "2020-12-19 11:19:32,083 INFO adding focus loss on mask_prob with a weighting of 1.0\n"
     ]
    }
   ],
   "source": [
    "if 'strategy' not in globals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "# inject the pre-trained unet if given, otherwise build the model without the pretrained unet\n",
    "with strategy.scope():\n",
    "    model = st.create_affine_cycle_transformer_model(config=config, unet=locals().get('unet', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"affine_cycle_transformer\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "input_1 (InputLayer)                             [(None, 40, 64, 64, 1)]          0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv_encoder (ConvEncoder)                       ((None, 2, 4, 4, 256), [(None, 4 3537424           input_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "global_average_pooling3d (GlobalAveragePooling3D (None, 256)                      0                 conv_encoder[0][0]                                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense1 (Dense)                                   (None, 256)                      65792             global_average_pooling3d[0][0]                    \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense2 (Dense)                                   (None, 9)                        2313              dense1[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (TensorFlowOpLayer)  [(None, 3)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (TensorFlowOpLayer)  [(None, 3)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "concatenate (Concatenate)                        (None, 6)                        0                 tf_op_layer_strided_slice_1[0][0]                 \n",
      "                                                                                                    tf_op_layer_strided_slice_2[0][0]                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (TensorFlowOpLayer)    [(None, 6)]                      0                 dense2[0][0]                                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_mod_matrix (Euler2Matrix)                 (None, 12)                       0                 concatenate[0][0]                                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_matrix (Euler2Matrix)                     (None, 12)                       0                 tf_op_layer_strided_slice[0][0]                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax_mod_st (SpatialTransformer)               (None, 40, 64, 64, 1)            0                 input_1[0][0]                                     \n",
      "                                                                                                    ax2sax_mod_matrix[0][0]                           \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "input_2 (InputLayer)                             [(None, 40, 64, 64, 1)]          0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "inverse3d_matrix (Inverse3DMatrix)               (None, 12)                       0                 ax2sax_matrix[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "mask_prob (UnetWrapper)                          (None, 40, 64, 64, 3)            19432275          ax2sax_mod_st[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "inverse3d_matrix_1 (Inverse3DMatrix)             (None, 12)                       0                 ax2sax_mod_matrix[0][0]                           \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "ax2sax (SpatialTransformer)                      (None, 40, 64, 64, 1)            0                 input_1[0][0]                                     \n",
      "                                                                                                    ax2sax_matrix[0][0]                               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "sax2ax (SpatialTransformer)                      (None, 40, 64, 64, 1)            0                 input_2[0][0]                                     \n",
      "                                                                                                    inverse3d_matrix[0][0]                            \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "mask2ax (SpatialTransformer)                     (None, 40, 64, 64, 3)            0                 mask_prob[0][0]                                   \n",
      "                                                                                                    inverse3d_matrix_1[0][0]                          \n",
      "======================================================================================================================================================\n",
      "Total params: 23,037,804\n",
      "Trainable params: 3,603,545\n",
      "Non-trainable params: 19,434,259\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=150)\n",
    "#plot_model(model, to_file='reports/figures/temp_graph.pdf',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea673b0e30454ff6af9b992c134b36bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='im', max=1), Text(value='0.001', description='mask_small…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def select_image_in_batch(im = (0,x.shape[0]- 1, 1),mask_smaller_than='0.001', slice_by=(1,6)):\n",
    "    global m\n",
    "    import numpy as np\n",
    "    temp = x[im]\n",
    "    sax = x2[im]\n",
    "    temp_ = y[im]\n",
    "    \n",
    "    mask = temp_ >float(mask_smaller_than)\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on: {}'.format(temp.shape))\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    pred, inv_pred, ax2sax_mod, prob, ax_msk,m, m_mod = model.predict(x = [np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])                     \n",
    "    logging.info('rotated by the model: {}'.format(pred[0].shape))\n",
    "    show_2D_or_3D(pred[0][::slice_by], mask[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverse rotation on SAX: {}'.format(inv_pred[0].shape))\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask: {}'.format(inv_pred[0].shape))\n",
    "    show_2D_or_3D(prob[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask in ax: {}'.format(ax_msk[0].shape))\n",
    "    show_2D_or_3D(ax_msk[0][::slice_by])\n",
    "    plt.show()\n",
    "    \n",
    "    # calculate the loss mask from target AX2SAX image\n",
    "    mask = temp_ >float(mask_smaller_than)\n",
    "    logging.info('masked by GT: {}'.format(mask.shape))\n",
    "    masked = pred[0] * mask\n",
    "    show_2D_or_3D(masked[::slice_by], mask[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (AX2SAX): {}'.format(temp_.shape))\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('Created MSE mask by thresholding the target (AX2SAX) with {}: {}'.format(mask_smaller_than,temp_.shape))\n",
    "    show_2D_or_3D(mask[::slice_by])\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        from tensorflow.keras.metrics import MSE as mse\n",
    "        logging.info('MSE: {}'.format(mse(pred[0], temp_).numpy().mean()))\n",
    "        logging.info('prob loss: {}'.format(metr.max_volume_loss(min_probabillity=0.5)(temp_[tf.newaxis,...],prob).numpy().mean()))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "        print(np.reshape(m_mod[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-19 11:20:03,271 INFO Fit model, start trainings process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 21.2872 - ax2sax_loss: 0.9152 - sax2ax_loss: 1.1140 - mask_prob_loss: 0.9956\n",
      "Epoch 00001: val_loss improved from inf to 21.84462, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 75s 1s/step - loss: 21.2872 - ax2sax_loss: 0.9152 - sax2ax_loss: 1.1140 - mask_prob_loss: 0.9956 - val_loss: 21.8446 - val_ax2sax_loss: 0.9704 - val_sax2ax_loss: 1.1147 - val_mask_prob_loss: 0.9935 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 20.1449 - ax2sax_loss: 0.8564 - sax2ax_loss: 1.0585 - mask_prob_loss: 0.9961\n",
      "Epoch 00002: val_loss improved from 21.84462 to 20.66088, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 52s 870ms/step - loss: 20.1449 - ax2sax_loss: 0.8564 - sax2ax_loss: 1.0585 - mask_prob_loss: 0.9961 - val_loss: 20.6609 - val_ax2sax_loss: 0.9110 - val_sax2ax_loss: 1.0557 - val_mask_prob_loss: 0.9935 - lr: 1.0000e-04\n",
      "Epoch 3/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 20.0315 - ax2sax_loss: 0.8512 - sax2ax_loss: 1.0524 - mask_prob_loss: 0.9960\n",
      "Epoch 00003: val_loss improved from 20.66088 to 20.42214, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 53s 881ms/step - loss: 20.0315 - ax2sax_loss: 0.8512 - sax2ax_loss: 1.0524 - mask_prob_loss: 0.9960 - val_loss: 20.4221 - val_ax2sax_loss: 0.8982 - val_sax2ax_loss: 1.0446 - val_mask_prob_loss: 0.9943 - lr: 1.0000e-04\n",
      "Epoch 4/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 19.7840 - ax2sax_loss: 0.8391 - sax2ax_loss: 1.0396 - mask_prob_loss: 0.9963\n",
      "Epoch 00004: val_loss improved from 20.42214 to 20.26443, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 54s 907ms/step - loss: 19.7840 - ax2sax_loss: 0.8391 - sax2ax_loss: 1.0396 - mask_prob_loss: 0.9963 - val_loss: 20.2644 - val_ax2sax_loss: 0.8924 - val_sax2ax_loss: 1.0346 - val_mask_prob_loss: 0.9943 - lr: 1.0000e-04\n",
      "Epoch 5/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 18.8589 - ax2sax_loss: 0.7965 - sax2ax_loss: 0.9898 - mask_prob_loss: 0.9962\n",
      "Epoch 00005: val_loss improved from 20.26443 to 19.51548, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 56s 941ms/step - loss: 18.8589 - ax2sax_loss: 0.7965 - sax2ax_loss: 0.9898 - mask_prob_loss: 0.9962 - val_loss: 19.5155 - val_ax2sax_loss: 0.8596 - val_sax2ax_loss: 0.9926 - val_mask_prob_loss: 0.9941 - lr: 1.0000e-04\n",
      "Epoch 6/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 17.5961 - ax2sax_loss: 0.7392 - sax2ax_loss: 0.9209 - mask_prob_loss: 0.9954\n",
      "Epoch 00006: val_loss improved from 19.51548 to 18.81228, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 58s 962ms/step - loss: 17.5961 - ax2sax_loss: 0.7392 - sax2ax_loss: 0.9209 - mask_prob_loss: 0.9954 - val_loss: 18.8123 - val_ax2sax_loss: 0.8228 - val_sax2ax_loss: 0.9590 - val_mask_prob_loss: 0.9942 - lr: 1.0000e-04\n",
      "Epoch 7/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 17.0883 - ax2sax_loss: 0.7129 - sax2ax_loss: 0.8964 - mask_prob_loss: 0.9955\n",
      "Epoch 00007: val_loss improved from 18.81228 to 18.49987, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 958ms/step - loss: 17.0883 - ax2sax_loss: 0.7129 - sax2ax_loss: 0.8964 - mask_prob_loss: 0.9955 - val_loss: 18.4999 - val_ax2sax_loss: 0.8063 - val_sax2ax_loss: 0.9443 - val_mask_prob_loss: 0.9942 - lr: 1.0000e-04\n",
      "Epoch 8/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.6799 - ax2sax_loss: 0.6912 - sax2ax_loss: 0.8772 - mask_prob_loss: 0.9959\n",
      "Epoch 00008: val_loss improved from 18.49987 to 18.22053, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 956ms/step - loss: 16.6799 - ax2sax_loss: 0.6912 - sax2ax_loss: 0.8772 - mask_prob_loss: 0.9959 - val_loss: 18.2205 - val_ax2sax_loss: 0.7913 - val_sax2ax_loss: 0.9313 - val_mask_prob_loss: 0.9943 - lr: 1.0000e-04\n",
      "Epoch 9/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.5116 - ax2sax_loss: 0.6810 - sax2ax_loss: 0.8706 - mask_prob_loss: 0.9963\n",
      "Epoch 00009: val_loss improved from 18.22053 to 18.09001, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 949ms/step - loss: 16.5116 - ax2sax_loss: 0.6810 - sax2ax_loss: 0.8706 - mask_prob_loss: 0.9963 - val_loss: 18.0900 - val_ax2sax_loss: 0.7842 - val_sax2ax_loss: 0.9253 - val_mask_prob_loss: 0.9945 - lr: 1.0000e-04\n",
      "Epoch 10/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4368 - ax2sax_loss: 0.6761 - sax2ax_loss: 0.8680 - mask_prob_loss: 0.9963\n",
      "Epoch 00010: val_loss improved from 18.09001 to 18.08386, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 949ms/step - loss: 16.4368 - ax2sax_loss: 0.6761 - sax2ax_loss: 0.8680 - mask_prob_loss: 0.9963 - val_loss: 18.0839 - val_ax2sax_loss: 0.7838 - val_sax2ax_loss: 0.9251 - val_mask_prob_loss: 0.9944 - lr: 1.0000e-04\n",
      "Epoch 11/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4373 - ax2sax_loss: 0.6756 - sax2ax_loss: 0.8685 - mask_prob_loss: 0.9965\n",
      "Epoch 00011: val_loss improved from 18.08386 to 17.83263, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 946ms/step - loss: 16.4373 - ax2sax_loss: 0.6756 - sax2ax_loss: 0.8685 - mask_prob_loss: 0.9965 - val_loss: 17.8326 - val_ax2sax_loss: 0.7742 - val_sax2ax_loss: 0.9096 - val_mask_prob_loss: 0.9951 - lr: 1.0000e-04\n",
      "Epoch 12/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4187 - ax2sax_loss: 0.6751 - sax2ax_loss: 0.8671 - mask_prob_loss: 0.9963\n",
      "Epoch 00012: val_loss did not improve from 17.83263\n",
      "60/60 [==============================] - 58s 964ms/step - loss: 16.4187 - ax2sax_loss: 0.6751 - sax2ax_loss: 0.8671 - mask_prob_loss: 0.9963 - val_loss: 17.8741 - val_ax2sax_loss: 0.7754 - val_sax2ax_loss: 0.9125 - val_mask_prob_loss: 0.9949 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4262 - ax2sax_loss: 0.6752 - sax2ax_loss: 0.8678 - mask_prob_loss: 0.9965\n",
      "Epoch 00013: val_loss did not improve from 17.83263\n",
      "60/60 [==============================] - 57s 957ms/step - loss: 16.4262 - ax2sax_loss: 0.6752 - sax2ax_loss: 0.8678 - mask_prob_loss: 0.9965 - val_loss: 17.8753 - val_ax2sax_loss: 0.7755 - val_sax2ax_loss: 0.9125 - val_mask_prob_loss: 0.9949 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4352 - ax2sax_loss: 0.6757 - sax2ax_loss: 0.8682 - mask_prob_loss: 0.9965\n",
      "Epoch 00014: val_loss improved from 17.83263 to 17.77179, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 947ms/step - loss: 16.4352 - ax2sax_loss: 0.6757 - sax2ax_loss: 0.8682 - mask_prob_loss: 0.9965 - val_loss: 17.7718 - val_ax2sax_loss: 0.7722 - val_sax2ax_loss: 0.9055 - val_mask_prob_loss: 0.9951 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4072 - ax2sax_loss: 0.6754 - sax2ax_loss: 0.8657 - mask_prob_loss: 0.9965\n",
      "Epoch 00015: val_loss did not improve from 17.77179\n",
      "60/60 [==============================] - 57s 957ms/step - loss: 16.4072 - ax2sax_loss: 0.6754 - sax2ax_loss: 0.8657 - mask_prob_loss: 0.9965 - val_loss: 17.8050 - val_ax2sax_loss: 0.7735 - val_sax2ax_loss: 0.9075 - val_mask_prob_loss: 0.9952 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4122 - ax2sax_loss: 0.6748 - sax2ax_loss: 0.8668 - mask_prob_loss: 0.9964\n",
      "Epoch 00016: val_loss did not improve from 17.77179\n",
      "60/60 [==============================] - 57s 946ms/step - loss: 16.4122 - ax2sax_loss: 0.6748 - sax2ax_loss: 0.8668 - mask_prob_loss: 0.9964 - val_loss: 17.8543 - val_ax2sax_loss: 0.7754 - val_sax2ax_loss: 0.9105 - val_mask_prob_loss: 0.9951 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4092 - ax2sax_loss: 0.6751 - sax2ax_loss: 0.8662 - mask_prob_loss: 0.9965\n",
      "Epoch 00017: val_loss did not improve from 17.77179\n",
      "60/60 [==============================] - 57s 954ms/step - loss: 16.4092 - ax2sax_loss: 0.6751 - sax2ax_loss: 0.8662 - mask_prob_loss: 0.9965 - val_loss: 17.7926 - val_ax2sax_loss: 0.7735 - val_sax2ax_loss: 0.9062 - val_mask_prob_loss: 0.9953 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4041 - ax2sax_loss: 0.6748 - sax2ax_loss: 0.8659 - mask_prob_loss: 0.9965\n",
      "Epoch 00018: val_loss did not improve from 17.77179\n",
      "60/60 [==============================] - 57s 947ms/step - loss: 16.4041 - ax2sax_loss: 0.6748 - sax2ax_loss: 0.8659 - mask_prob_loss: 0.9965 - val_loss: 17.7753 - val_ax2sax_loss: 0.7727 - val_sax2ax_loss: 0.9053 - val_mask_prob_loss: 0.9953 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3987 - ax2sax_loss: 0.6746 - sax2ax_loss: 0.8656 - mask_prob_loss: 0.9966\n",
      "Epoch 00019: val_loss improved from 17.77179 to 17.71315, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 952ms/step - loss: 16.3987 - ax2sax_loss: 0.6746 - sax2ax_loss: 0.8656 - mask_prob_loss: 0.9966 - val_loss: 17.7132 - val_ax2sax_loss: 0.7701 - val_sax2ax_loss: 0.9017 - val_mask_prob_loss: 0.9955 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3930 - ax2sax_loss: 0.6747 - sax2ax_loss: 0.8649 - mask_prob_loss: 0.9966\n",
      "Epoch 00020: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 56s 940ms/step - loss: 16.3930 - ax2sax_loss: 0.6747 - sax2ax_loss: 0.8649 - mask_prob_loss: 0.9966 - val_loss: 17.7521 - val_ax2sax_loss: 0.7714 - val_sax2ax_loss: 0.9042 - val_mask_prob_loss: 0.9954 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4065 - ax2sax_loss: 0.6744 - sax2ax_loss: 0.8666 - mask_prob_loss: 0.9966\n",
      "Epoch 00021: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 57s 947ms/step - loss: 16.4065 - ax2sax_loss: 0.6744 - sax2ax_loss: 0.8666 - mask_prob_loss: 0.9966 - val_loss: 17.8100 - val_ax2sax_loss: 0.7738 - val_sax2ax_loss: 0.9077 - val_mask_prob_loss: 0.9953 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4137 - ax2sax_loss: 0.6750 - sax2ax_loss: 0.8667 - mask_prob_loss: 0.9967\n",
      "Epoch 00022: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 56s 936ms/step - loss: 16.4137 - ax2sax_loss: 0.6750 - sax2ax_loss: 0.8667 - mask_prob_loss: 0.9967 - val_loss: 17.7482 - val_ax2sax_loss: 0.7722 - val_sax2ax_loss: 0.9031 - val_mask_prob_loss: 0.9955 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4042 - ax2sax_loss: 0.6746 - sax2ax_loss: 0.8661 - mask_prob_loss: 0.9966\n",
      "Epoch 00023: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 56s 937ms/step - loss: 16.4042 - ax2sax_loss: 0.6746 - sax2ax_loss: 0.8661 - mask_prob_loss: 0.9966 - val_loss: 17.8325 - val_ax2sax_loss: 0.7748 - val_sax2ax_loss: 0.9089 - val_mask_prob_loss: 0.9952 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.4148 - ax2sax_loss: 0.6753 - sax2ax_loss: 0.8665 - mask_prob_loss: 0.9966\n",
      "Epoch 00024: val_loss did not improve from 17.71315\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.9999999242136255e-05.\n",
      "60/60 [==============================] - 57s 946ms/step - loss: 16.4148 - ax2sax_loss: 0.6753 - sax2ax_loss: 0.8665 - mask_prob_loss: 0.9966 - val_loss: 17.8794 - val_ax2sax_loss: 0.7772 - val_sax2ax_loss: 0.9112 - val_mask_prob_loss: 0.9952 - lr: 3.0000e-05\n",
      "Epoch 25/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3885 - ax2sax_loss: 0.6746 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00025: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 57s 950ms/step - loss: 16.3885 - ax2sax_loss: 0.6746 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.7298 - val_ax2sax_loss: 0.7709 - val_sax2ax_loss: 0.9025 - val_mask_prob_loss: 0.9955 - lr: 3.0000e-05\n",
      "Epoch 26/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3842 - ax2sax_loss: 0.6742 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00026: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 57s 944ms/step - loss: 16.3842 - ax2sax_loss: 0.6742 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.7648 - val_ax2sax_loss: 0.7723 - val_sax2ax_loss: 0.9046 - val_mask_prob_loss: 0.9954 - lr: 3.0000e-05\n",
      "Epoch 27/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3881 - ax2sax_loss: 0.6743 - sax2ax_loss: 0.8649 - mask_prob_loss: 0.9966\n",
      "Epoch 00027: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 56s 936ms/step - loss: 16.3881 - ax2sax_loss: 0.6743 - sax2ax_loss: 0.8649 - mask_prob_loss: 0.9966 - val_loss: 17.7510 - val_ax2sax_loss: 0.7718 - val_sax2ax_loss: 0.9037 - val_mask_prob_loss: 0.9955 - lr: 3.0000e-05\n",
      "Epoch 28/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3893 - ax2sax_loss: 0.6743 - sax2ax_loss: 0.8650 - mask_prob_loss: 0.9966\n",
      "Epoch 00028: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 59s 980ms/step - loss: 16.3893 - ax2sax_loss: 0.6743 - sax2ax_loss: 0.8650 - mask_prob_loss: 0.9966 - val_loss: 17.7203 - val_ax2sax_loss: 0.7703 - val_sax2ax_loss: 0.9021 - val_mask_prob_loss: 0.9955 - lr: 3.0000e-05\n",
      "Epoch 29/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3875 - ax2sax_loss: 0.6742 - sax2ax_loss: 0.8648 - mask_prob_loss: 0.9967\n",
      "Epoch 00029: val_loss did not improve from 17.71315\n",
      "60/60 [==============================] - 58s 960ms/step - loss: 16.3875 - ax2sax_loss: 0.6742 - sax2ax_loss: 0.8648 - mask_prob_loss: 0.9967 - val_loss: 17.7168 - val_ax2sax_loss: 0.7703 - val_sax2ax_loss: 0.9019 - val_mask_prob_loss: 0.9955 - lr: 3.0000e-05\n",
      "Epoch 30/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3840 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967\n",
      "Epoch 00030: val_loss did not improve from 17.71315\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
      "60/60 [==============================] - 58s 964ms/step - loss: 16.3840 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967 - val_loss: 17.7136 - val_ax2sax_loss: 0.7699 - val_sax2ax_loss: 0.9019 - val_mask_prob_loss: 0.9955 - lr: 9.0000e-06\n",
      "Epoch 31/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3823 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00031: val_loss improved from 17.71315 to 17.68666, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 946ms/step - loss: 16.3823 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6867 - val_ax2sax_loss: 0.7690 - val_sax2ax_loss: 0.9002 - val_mask_prob_loss: 0.9956 - lr: 9.0000e-06\n",
      "Epoch 32/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3834 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967\n",
      "Epoch 00032: val_loss improved from 17.68666 to 17.63498, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 950ms/step - loss: 16.3834 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967 - val_loss: 17.6350 - val_ax2sax_loss: 0.7667 - val_sax2ax_loss: 0.8973 - val_mask_prob_loss: 0.9956 - lr: 9.0000e-06\n",
      "Epoch 33/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3816 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8644 - mask_prob_loss: 0.9966\n",
      "Epoch 00033: val_loss did not improve from 17.63498\n",
      "60/60 [==============================] - 57s 958ms/step - loss: 16.3816 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8644 - mask_prob_loss: 0.9966 - val_loss: 17.6426 - val_ax2sax_loss: 0.7671 - val_sax2ax_loss: 0.8976 - val_mask_prob_loss: 0.9956 - lr: 9.0000e-06\n",
      "Epoch 34/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3854 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8648 - mask_prob_loss: 0.9967\n",
      "Epoch 00034: val_loss did not improve from 17.63498\n",
      "60/60 [==============================] - 58s 960ms/step - loss: 16.3854 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8648 - mask_prob_loss: 0.9967 - val_loss: 17.6896 - val_ax2sax_loss: 0.7690 - val_sax2ax_loss: 0.9004 - val_mask_prob_loss: 0.9955 - lr: 9.0000e-06\n",
      "Epoch 35/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3844 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9967\n",
      "Epoch 00035: val_loss did not improve from 17.63498\n",
      "60/60 [==============================] - 57s 958ms/step - loss: 16.3844 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9967 - val_loss: 17.6711 - val_ax2sax_loss: 0.7685 - val_sax2ax_loss: 0.8990 - val_mask_prob_loss: 0.9956 - lr: 9.0000e-06\n",
      "Epoch 36/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3829 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00036: val_loss did not improve from 17.63498\n",
      "60/60 [==============================] - 56s 937ms/step - loss: 16.3829 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.7427 - val_ax2sax_loss: 0.7713 - val_sax2ax_loss: 0.9034 - val_mask_prob_loss: 0.9955 - lr: 9.0000e-06\n",
      "Epoch 37/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3852 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966\n",
      "Epoch 00037: val_loss did not improve from 17.63498\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
      "60/60 [==============================] - 57s 943ms/step - loss: 16.3852 - ax2sax_loss: 0.6741 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966 - val_loss: 17.7670 - val_ax2sax_loss: 0.7724 - val_sax2ax_loss: 0.9047 - val_mask_prob_loss: 0.9954 - lr: 2.7000e-06\n",
      "Epoch 38/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3804 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8644 - mask_prob_loss: 0.9967\n",
      "Epoch 00038: val_loss did not improve from 17.63498\n",
      "60/60 [==============================] - 57s 953ms/step - loss: 16.3804 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8644 - mask_prob_loss: 0.9967 - val_loss: 17.7437 - val_ax2sax_loss: 0.7713 - val_sax2ax_loss: 0.9035 - val_mask_prob_loss: 0.9955 - lr: 2.7000e-06\n",
      "Epoch 39/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3802 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8644 - mask_prob_loss: 0.9966\n",
      "Epoch 00039: val_loss improved from 17.63498 to 17.62880, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 946ms/step - loss: 16.3802 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8644 - mask_prob_loss: 0.9966 - val_loss: 17.6288 - val_ax2sax_loss: 0.7665 - val_sax2ax_loss: 0.8968 - val_mask_prob_loss: 0.9956 - lr: 2.7000e-06\n",
      "Epoch 40/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3827 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00040: val_loss did not improve from 17.62880\n",
      "60/60 [==============================] - 56s 940ms/step - loss: 16.3827 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.6792 - val_ax2sax_loss: 0.7686 - val_sax2ax_loss: 0.8998 - val_mask_prob_loss: 0.9956 - lr: 2.7000e-06\n",
      "Epoch 41/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3814 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00041: val_loss did not improve from 17.62880\n",
      "60/60 [==============================] - 56s 941ms/step - loss: 16.3814 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6312 - val_ax2sax_loss: 0.7665 - val_sax2ax_loss: 0.8971 - val_mask_prob_loss: 0.9956 - lr: 2.7000e-06\n",
      "Epoch 42/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3820 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967\n",
      "Epoch 00042: val_loss improved from 17.62880 to 17.58346, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 57s 945ms/step - loss: 16.3820 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967 - val_loss: 17.5835 - val_ax2sax_loss: 0.7645 - val_sax2ax_loss: 0.8943 - val_mask_prob_loss: 0.9957 - lr: 2.7000e-06\n",
      "Epoch 43/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3818 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00043: val_loss did not improve from 17.58346\n",
      "60/60 [==============================] - 58s 965ms/step - loss: 16.3818 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.7099 - val_ax2sax_loss: 0.7699 - val_sax2ax_loss: 0.9015 - val_mask_prob_loss: 0.9955 - lr: 2.7000e-06\n",
      "Epoch 44/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3827 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9968\n",
      "Epoch 00044: val_loss did not improve from 17.58346\n",
      "60/60 [==============================] - 57s 947ms/step - loss: 16.3827 - ax2sax_loss: 0.6740 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9968 - val_loss: 17.7464 - val_ax2sax_loss: 0.7715 - val_sax2ax_loss: 0.9036 - val_mask_prob_loss: 0.9954 - lr: 2.7000e-06\n",
      "Epoch 45/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3821 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00045: val_loss did not improve from 17.58346\n",
      "60/60 [==============================] - 56s 937ms/step - loss: 16.3821 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.7063 - val_ax2sax_loss: 0.7698 - val_sax2ax_loss: 0.9012 - val_mask_prob_loss: 0.9955 - lr: 2.7000e-06\n",
      "Epoch 46/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3815 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00046: val_loss did not improve from 17.58346\n",
      "60/60 [==============================] - 56s 935ms/step - loss: 16.3815 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.7499 - val_ax2sax_loss: 0.7715 - val_sax2ax_loss: 0.9039 - val_mask_prob_loss: 0.9955 - lr: 2.7000e-06\n",
      "Epoch 47/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3828 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966\n",
      "Epoch 00047: val_loss did not improve from 17.58346\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 8.099999604382901e-07.\n",
      "60/60 [==============================] - 57s 951ms/step - loss: 16.3828 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966 - val_loss: 17.6445 - val_ax2sax_loss: 0.7671 - val_sax2ax_loss: 0.8978 - val_mask_prob_loss: 0.9956 - lr: 8.1000e-07\n",
      "Epoch 48/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3824 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966\n",
      "Epoch 00048: val_loss improved from 17.58346 to 17.57211, saving model to models/3D/ax_sax/trained_on_original_ax_sax_pairs_lower_clipping/2020-12-19_11_14/model.h5\n",
      "60/60 [==============================] - 58s 959ms/step - loss: 16.3824 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966 - val_loss: 17.5721 - val_ax2sax_loss: 0.7640 - val_sax2ax_loss: 0.8937 - val_mask_prob_loss: 0.9957 - lr: 8.1000e-07\n",
      "Epoch 49/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3825 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966\n",
      "Epoch 00049: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 946ms/step - loss: 16.3825 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966 - val_loss: 17.6913 - val_ax2sax_loss: 0.7691 - val_sax2ax_loss: 0.9005 - val_mask_prob_loss: 0.9956 - lr: 8.1000e-07\n",
      "Epoch 50/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3826 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966\n",
      "Epoch 00050: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 954ms/step - loss: 16.3826 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966 - val_loss: 17.6934 - val_ax2sax_loss: 0.7691 - val_sax2ax_loss: 0.9007 - val_mask_prob_loss: 0.9955 - lr: 8.1000e-07\n",
      "Epoch 51/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3815 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00051: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 944ms/step - loss: 16.3815 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.6231 - val_ax2sax_loss: 0.7662 - val_sax2ax_loss: 0.8966 - val_mask_prob_loss: 0.9956 - lr: 8.1000e-07\n",
      "Epoch 52/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3823 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9967\n",
      "Epoch 00052: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 945ms/step - loss: 16.3823 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9967 - val_loss: 17.7323 - val_ax2sax_loss: 0.7709 - val_sax2ax_loss: 0.9028 - val_mask_prob_loss: 0.9955 - lr: 8.1000e-07\n",
      "Epoch 53/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3824 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9967\n",
      "Epoch 00053: val_loss did not improve from 17.57211\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 2.4299998813148704e-07.\n",
      "60/60 [==============================] - 56s 936ms/step - loss: 16.3824 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9967 - val_loss: 17.7797 - val_ax2sax_loss: 0.7727 - val_sax2ax_loss: 0.9057 - val_mask_prob_loss: 0.9954 - lr: 2.4300e-07\n",
      "Epoch 54/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3821 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966\n",
      "Epoch 00054: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 942ms/step - loss: 16.3821 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8647 - mask_prob_loss: 0.9966 - val_loss: 17.7620 - val_ax2sax_loss: 0.7720 - val_sax2ax_loss: 0.9047 - val_mask_prob_loss: 0.9954 - lr: 2.4300e-07\n",
      "Epoch 55/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3818 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00055: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 950ms/step - loss: 16.3818 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.6383 - val_ax2sax_loss: 0.7668 - val_sax2ax_loss: 0.8975 - val_mask_prob_loss: 0.9956 - lr: 2.4300e-07\n",
      "Epoch 56/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3817 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966\n",
      "Epoch 00056: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 949ms/step - loss: 16.3817 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9966 - val_loss: 17.5939 - val_ax2sax_loss: 0.7650 - val_sax2ax_loss: 0.8949 - val_mask_prob_loss: 0.9957 - lr: 2.4300e-07\n",
      "Epoch 57/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3814 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967\n",
      "Epoch 00057: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 951ms/step - loss: 16.3814 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8646 - mask_prob_loss: 0.9967 - val_loss: 17.5932 - val_ax2sax_loss: 0.7649 - val_sax2ax_loss: 0.8949 - val_mask_prob_loss: 0.9956 - lr: 2.4300e-07\n",
      "Epoch 58/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00058: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 953ms/step - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.5808 - val_ax2sax_loss: 0.7643 - val_sax2ax_loss: 0.8942 - val_mask_prob_loss: 0.9957 - lr: 2.4300e-07\n",
      "Epoch 59/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966\n",
      "Epoch 00059: val_loss did not improve from 17.57211\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 7.289999643944612e-08.\n",
      "60/60 [==============================] - 57s 943ms/step - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966 - val_loss: 17.6003 - val_ax2sax_loss: 0.7653 - val_sax2ax_loss: 0.8952 - val_mask_prob_loss: 0.9957 - lr: 7.2900e-08\n",
      "Epoch 60/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966\n",
      "Epoch 00060: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 56s 941ms/step - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966 - val_loss: 17.6230 - val_ax2sax_loss: 0.7662 - val_sax2ax_loss: 0.8965 - val_mask_prob_loss: 0.9956 - lr: 7.2900e-08\n",
      "Epoch 61/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3803 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00061: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 56s 939ms/step - loss: 16.3803 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6824 - val_ax2sax_loss: 0.7688 - val_sax2ax_loss: 0.8999 - val_mask_prob_loss: 0.9955 - lr: 7.2900e-08\n",
      "Epoch 62/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00062: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 56s 939ms/step - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6390 - val_ax2sax_loss: 0.7668 - val_sax2ax_loss: 0.8976 - val_mask_prob_loss: 0.9956 - lr: 7.2900e-08\n",
      "Epoch 63/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3803 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966\n",
      "Epoch 00063: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 943ms/step - loss: 16.3803 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966 - val_loss: 17.6488 - val_ax2sax_loss: 0.7671 - val_sax2ax_loss: 0.8982 - val_mask_prob_loss: 0.9956 - lr: 7.2900e-08\n",
      "Epoch 64/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966\n",
      "Epoch 00064: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 56s 936ms/step - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9966 - val_loss: 17.6610 - val_ax2sax_loss: 0.7676 - val_sax2ax_loss: 0.8989 - val_mask_prob_loss: 0.9955 - lr: 7.2900e-08\n",
      "Epoch 65/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00065: val_loss did not improve from 17.57211\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 2.1869998079182552e-08.\n",
      "60/60 [==============================] - 57s 948ms/step - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6994 - val_ax2sax_loss: 0.7694 - val_sax2ax_loss: 0.9010 - val_mask_prob_loss: 0.9955 - lr: 2.1870e-08\n",
      "Epoch 66/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00066: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 949ms/step - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6841 - val_ax2sax_loss: 0.7688 - val_sax2ax_loss: 0.9001 - val_mask_prob_loss: 0.9955 - lr: 2.1870e-08\n",
      "Epoch 67/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00067: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 954ms/step - loss: 16.3804 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.6714 - val_ax2sax_loss: 0.7681 - val_sax2ax_loss: 0.8995 - val_mask_prob_loss: 0.9956 - lr: 2.1870e-08\n",
      "Epoch 68/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967\n",
      "Epoch 00068: val_loss did not improve from 17.57211\n",
      "60/60 [==============================] - 57s 953ms/step - loss: 16.3805 - ax2sax_loss: 0.6739 - sax2ax_loss: 0.8645 - mask_prob_loss: 0.9967 - val_loss: 17.5901 - val_ax2sax_loss: 0.7647 - val_sax2ax_loss: 0.8947 - val_mask_prob_loss: 0.9957 - lr: 2.1870e-08\n",
      "Epoch 00068: early stopping\n"
     ]
    }
   ],
   "source": [
    "# train one model\n",
    "initial_epoch = 0\n",
    "logging.info('Fit model, start trainings process')\n",
    "# fit model with trainingsgenerator\n",
    "results = model.fit(\n",
    "    x=batch_generator,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=len(valid_generator),\n",
    "    epochs=200,\n",
    "    callbacks = get_callbacks(config, valid_generator),\n",
    "    steps_per_epoch = len(batch_generator),\n",
    "    initial_epoch=initial_epoch,\n",
    "    max_queue_size=20,\n",
    "    workers=2,\n",
    "    use_multiprocessing=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if, for any reason, you want to save the latest model, use this cell\n",
    "#tf.keras.models.save_model(model,filepath=config['MODEL_PATH'],overwrite=True,include_optimizer=False,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['MODEL_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 14:30:29,323 INFO Load model from Experiment: 3D/ax_sax/unetwithdownsamplingaugmentation_new_data\n",
      "2020-12-07 14:30:30,068 INFO unet given, use it to max probability\n",
      "2020-12-07 14:30:48,204 INFO adding ax2sax MSE loss with a weighting of 10.0\n",
      "2020-12-07 14:30:48,205 INFO adding cycle loss with a weighting of 10.0\n",
      "2020-12-07 14:30:48,206 INFO adding focus loss on mask_prob with a weighting of 1.0\n",
      "2020-12-07 14:30:48,484 INFO loaded model weights as h5 file\n"
     ]
    }
   ],
   "source": [
    "# Fast tests of a trained model, the \"real\" predictions will be done in src/notebooks/Predict\n",
    "\n",
    "\"\"\"\n",
    "load past config for model training \n",
    "\"\"\"\n",
    "if 'strategy' not in locals():\n",
    "    # distribute the training with the mirrored data paradigm across multiple gpus if available, if not use gpu 0\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=config.get('GPUS', [\"/gpu:0\"]))\n",
    "\n",
    "# round the crop and pad values instead of ceil\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-03_18_20/config.json' # Fold 0\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-03_22_02/config.json' # Fold 1\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-04_16_56/config.json' # Fold 2\n",
    "#config_file = 'reports/configs/3D/ax_sax/unetwithdownsamplingaugmentation_new_data/2020-12-07_12_36/config.json' # Fold 3\n",
    "\n",
    "config_file = 'reports/configs/3D/ax_sax/train_on_ax_sax/fold0/2020-12-17_11_44/config.json' # Fold 0\n",
    "\n",
    "\n",
    "\n",
    "# load a pre-trained ax2sax model, create the graph and load the weights separately, due to own loss functions, this is easier\n",
    "with open(config_file, encoding='utf-8') as data_file:\n",
    "    config_temp = json.loads(data_file.read())\n",
    "config_temp['LOSS_FUNCTION'] = config['LOSS_FUNCTION']\n",
    "logging.info('Load model from Experiment: {}'.format(config_temp['EXPERIMENT']))\n",
    "\n",
    "with strategy.scope():\n",
    "    globals()['model'] = st.create_affine_cycle_transformer_model(config=config_temp, metrics=metrics, unet=locals().get('unet', None))\n",
    "    model.load_weights(os.path.join(config_temp['MODEL_PATH'],'model.h5'))\n",
    "    logging.info('loaded model weights as h5 file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast predictions with all files of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 20:07:27,707 INFO Create DataGenerator\n",
      "2020-12-03 20:07:27,708 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 10\n",
      " Scaler: MinMax\n",
      " Images: 120 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-03 20:07:27,709 INFO No augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcda926c12e41d386c1bac03825d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='im', max=9), IntSlider(value=3, description='slice_by', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict, visualise the transformation of AX train files\n",
    "import numpy as np\n",
    "cfg = config.copy()\n",
    "cfg['BATCHSIZE'] = 10\n",
    "cfg['AUGMENT_GRID'] = False\n",
    "valid_generator = CycleMotionDataGenerator(x_train_ax, x_train_sax, cfg)\n",
    "input_, output_ = valid_generator.__getitem__(0)\n",
    "x_ = input_[0]\n",
    "x2_ = input_[1]\n",
    "y_ = output_[0]\n",
    "y2_ = output_[1]\n",
    "@interact\n",
    "def select_image_in_batch(im = (0,x_.shape[0]- 1, 1), slice_by=(1,6)):\n",
    "    global m\n",
    "    temp = x_[im]\n",
    "    temp_ = y_[im]\n",
    "    sax = x2_[im]\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on:')\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    pred, inv_pred, ax2sax_mod, pred_mask, ax2sax_msk,m_first, m = model.predict(x=[np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])\n",
    "\n",
    "    logging.info('rotated by the model')\n",
    "    show_2D_or_3D(pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('modified rotation of the model')\n",
    "    show_2D_or_3D(ax2sax_mod[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask:')\n",
    "    show_2D_or_3D(pred_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (SAX):')\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverted rotation on SAX')\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(np.reshape(m_first[0],(3,4)))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on the heldout test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 20:08:19,977 INFO Create DataGenerator\n",
      "2020-12-03 20:08:19,977 INFO Datagenerator created with: \n",
      " shape: [80, 112, 112]\n",
      " spacing: [3, 3, 3]\n",
      " batchsize: 42\n",
      " Scaler: MinMax\n",
      " Images: 42 \n",
      " Augment_grid: False \n",
      " Thread workers: 2\n",
      "2020-12-03 20:08:19,978 INFO No augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bde9676ae554b3e925284ca81a94a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='im', max=41), IntSlider(value=3, description='slice_by'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = config.copy()\n",
    "cfg['BATCHSIZE'] = len(x_val_ax)\n",
    "v_generator = CycleMotionDataGenerator(x_val_ax, x_val_sax, cfg)\n",
    "input_, output_ = v_generator.__getitem__(0)\n",
    "x_ = input_[0]\n",
    "x2_ = input_[1]\n",
    "y_ = output_[0]\n",
    "y2_ = output_[1]\n",
    "@interact\n",
    "def select_image_in_batch(im = (0,x_.shape[0]- 1, 1), slice_by=(1,6)):\n",
    "    global m\n",
    "    temp = x_[im]\n",
    "    temp_ = y_[im]\n",
    "    sax = x2_[im]\n",
    "    # define a different logging level to make the generator steps visible\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info('prediction on:')\n",
    "    show_2D_or_3D(temp[::slice_by])\n",
    "    plt.show()\n",
    "    \n",
    "    pred, inv_pred, ax2sax_mod, pred_mask, ax_mask, m_first, m = model.predict(x=[np.expand_dims(temp,axis=0),np.expand_dims(sax,axis=0)])\n",
    "    logging.info('rotated by the model')\n",
    "    show_2D_or_3D(pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('modified rotation')\n",
    "    show_2D_or_3D(ax2sax_mod[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted mask')\n",
    "    show_2D_or_3D(pred_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('predicted in AX')\n",
    "    show_2D_or_3D(ax_mask[0][::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('target (SAX):')\n",
    "    show_2D_or_3D(temp_[::slice_by])\n",
    "    plt.show()\n",
    "    logging.info('inverted rotation on SAX')\n",
    "    show_2D_or_3D(inv_pred[0][::slice_by])\n",
    "    plt.show()\n",
    "    try:\n",
    "        print(np.reshape(m_first[0],(3,4)))\n",
    "        print(np.reshape(m[0],(3,4)))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the memory usage\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ax2sax",
   "language": "python",
   "name": "ax2sax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
